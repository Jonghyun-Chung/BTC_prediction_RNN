{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "desirable-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "modern-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('bitcoin_transactions.csv')\n",
    "df = pd.DataFrame (df,columns=['date', 'open', 'high', 'low', 'close', 'volume', 'market_cap'])\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "df['year']= df['date'].dt.year\n",
    "df['month']= df['date'].dt.month\n",
    "df['day']= df['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "martial-reservation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>46,194.02</td>\n",
       "      <td>46,716.43</td>\n",
       "      <td>43,241.62</td>\n",
       "      <td>45,137.77</td>\n",
       "      <td>53,443,887,451</td>\n",
       "      <td>841,428,977,515</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-27</td>\n",
       "      <td>46,344.77</td>\n",
       "      <td>48,253.27</td>\n",
       "      <td>45,269.03</td>\n",
       "      <td>46,188.45</td>\n",
       "      <td>45,910,946,382</td>\n",
       "      <td>860,978,135,421</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-26</td>\n",
       "      <td>47,180.46</td>\n",
       "      <td>48,370.79</td>\n",
       "      <td>44,454.84</td>\n",
       "      <td>46,339.76</td>\n",
       "      <td>350,967,941,479</td>\n",
       "      <td>863,752,275,053</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>49,709.08</td>\n",
       "      <td>51,948.97</td>\n",
       "      <td>47,093.85</td>\n",
       "      <td>47,093.85</td>\n",
       "      <td>54,506,565,949</td>\n",
       "      <td>877,766,126,138</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>48,835.09</td>\n",
       "      <td>51,290.14</td>\n",
       "      <td>47,213.50</td>\n",
       "      <td>49,705.33</td>\n",
       "      <td>63,695,521,388</td>\n",
       "      <td>926,393,090,751</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>431.86</td>\n",
       "      <td>431.86</td>\n",
       "      <td>426.34</td>\n",
       "      <td>429.11</td>\n",
       "      <td>34,042,500</td>\n",
       "      <td>6,458,942,098</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>433.07</td>\n",
       "      <td>434.18</td>\n",
       "      <td>429.68</td>\n",
       "      <td>431.96</td>\n",
       "      <td>34,522,600</td>\n",
       "      <td>6,500,393,256</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>430.06</td>\n",
       "      <td>434.52</td>\n",
       "      <td>429.08</td>\n",
       "      <td>433.09</td>\n",
       "      <td>38,477,500</td>\n",
       "      <td>6,515,713,340</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>433.58</td>\n",
       "      <td>433.74</td>\n",
       "      <td>424.71</td>\n",
       "      <td>430.01</td>\n",
       "      <td>39,633,800</td>\n",
       "      <td>6,467,429,942</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>434.62</td>\n",
       "      <td>436.06</td>\n",
       "      <td>431.87</td>\n",
       "      <td>433.44</td>\n",
       "      <td>30,096,600</td>\n",
       "      <td>6,517,390,487</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1885 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date       open       high        low      close           volume  \\\n",
       "0    2021-02-28  46,194.02  46,716.43  43,241.62  45,137.77   53,443,887,451   \n",
       "1    2021-02-27  46,344.77  48,253.27  45,269.03  46,188.45   45,910,946,382   \n",
       "2    2021-02-26  47,180.46  48,370.79  44,454.84  46,339.76  350,967,941,479   \n",
       "3    2021-02-25  49,709.08  51,948.97  47,093.85  47,093.85   54,506,565,949   \n",
       "4    2021-02-24  48,835.09  51,290.14  47,213.50  49,705.33   63,695,521,388   \n",
       "...         ...        ...        ...        ...        ...              ...   \n",
       "1880 2016-01-06     431.86     431.86     426.34     429.11       34,042,500   \n",
       "1881 2016-01-05     433.07     434.18     429.68     431.96       34,522,600   \n",
       "1882 2016-01-04     430.06     434.52     429.08     433.09       38,477,500   \n",
       "1883 2016-01-03     433.58     433.74     424.71     430.01       39,633,800   \n",
       "1884 2016-01-02     434.62     436.06     431.87     433.44       30,096,600   \n",
       "\n",
       "           market_cap  year  month  day  \n",
       "0     841,428,977,515  2021      2   28  \n",
       "1     860,978,135,421  2021      2   27  \n",
       "2     863,752,275,053  2021      2   26  \n",
       "3     877,766,126,138  2021      2   25  \n",
       "4     926,393,090,751  2021      2   24  \n",
       "...               ...   ...    ...  ...  \n",
       "1880    6,458,942,098  2016      1    6  \n",
       "1881    6,500,393,256  2016      1    5  \n",
       "1882    6,515,713,340  2016      1    4  \n",
       "1883    6,467,429,942  2016      1    3  \n",
       "1884    6,517,390,487  2016      1    2  \n",
       "\n",
       "[1885 rows x 10 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "promotional-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['open']=df['open'].apply(lambda x: x.replace(',',''))\n",
    "df['high']=df['high'].apply(lambda x: x.replace(',',''))\n",
    "df['low']=df['low'].apply(lambda x: x.replace(',',''))\n",
    "df['close']=df['close'].apply(lambda x: x.replace(',',''))\n",
    "df['volume']=df['volume'].apply(lambda x: x.replace(',',''))\n",
    "df['market_cap']=df['market_cap'].apply(lambda x: x.replace(',',''))\n",
    "\n",
    "df['open'] = df['open'].astype(float)\n",
    "df['high'] = df['high'].astype(float)\n",
    "df['low'] = df['low'].astype(float)\n",
    "df['close'] = df['close'].astype(float)\n",
    "df['volume'] = df['volume'].astype(float)\n",
    "df['market_cap'] = df['market_cap'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "everyday-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('date', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "decent-blend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.to_numpy of           open      high       low     close        volume    market_cap  \\\n",
       "0     46194.02  46716.43  43241.62  45137.77  5.344389e+10  8.414290e+11   \n",
       "1     46344.77  48253.27  45269.03  46188.45  4.591095e+10  8.609781e+11   \n",
       "2     47180.46  48370.79  44454.84  46339.76  3.509679e+11  8.637523e+11   \n",
       "3     49709.08  51948.97  47093.85  47093.85  5.450657e+10  8.777661e+11   \n",
       "4     48835.09  51290.14  47213.50  49705.33  6.369552e+10  9.263931e+11   \n",
       "...        ...       ...       ...       ...           ...           ...   \n",
       "1880    431.86    431.86    426.34    429.11  3.404250e+07  6.458942e+09   \n",
       "1881    433.07    434.18    429.68    431.96  3.452260e+07  6.500393e+09   \n",
       "1882    430.06    434.52    429.08    433.09  3.847750e+07  6.515713e+09   \n",
       "1883    433.58    433.74    424.71    430.01  3.963380e+07  6.467430e+09   \n",
       "1884    434.62    436.06    431.87    433.44  3.009660e+07  6.517390e+09   \n",
       "\n",
       "      year  month  day  \n",
       "0     2021      2   28  \n",
       "1     2021      2   27  \n",
       "2     2021      2   26  \n",
       "3     2021      2   25  \n",
       "4     2021      2   24  \n",
       "...    ...    ...  ...  \n",
       "1880  2016      1    6  \n",
       "1881  2016      1    5  \n",
       "1882  2016      1    4  \n",
       "1883  2016      1    3  \n",
       "1884  2016      1    2  \n",
       "\n",
       "[1885 rows x 9 columns]>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "upper-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = df.to_numpy()\n",
    "xy = xy[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "spare-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [0,1,2,4,5,6,7,8,3]\n",
    "xy = xy[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "handy-elements",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  433.44,   430.01,   433.09, ..., 46339.76, 46188.45, 45137.77])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "theoretical-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(xy) * 0.9)\n",
    "train_set = xy[0:train_size]\n",
    "test_set = xy[train_size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "controlling-silly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696 189\n"
     ]
    }
   ],
   "source": [
    "train_size\n",
    "print(len(train_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "appropriate-indicator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1696,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_set[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "elegant-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = train_size\n",
    "data_dim = 9\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "statistical-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(seq_list):\n",
    "    X_data = seq_list[:,:-1]\n",
    "    y_data = seq_list[:,-1]\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "friendly-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = build_dataset(train_set)\n",
    "X_test, y_test = build_dataset(test_set)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc_x = MinMaxScaler()\n",
    "sc_y = MinMaxScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "y_train = np.reshape(y_train,(len(y_train),1))\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "\n",
    "X_test = sc_x.fit_transform(X_test)\n",
    "y_test = np.reshape(y_test,(len(y_test),1))\n",
    "y_test = sc_y.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fiscal-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 8) (189, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_test),np.shape(y_test))\n",
    "X_train= np.reshape(X_train, (len(X_train), 8, 1))\n",
    "X_test= np.reshape(X_test, (len(X_test), 8, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "worthy-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 8, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "distinct-habitat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-63-5fb64fc5de6e>:2: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-63-5fb64fc5de6e>:3: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/JonghyunChung/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/JonghyunChung/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ffd9b296390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ffd9b296390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ffd9b296390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ffd9b296390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffd7c639350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffd7c639350>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffd7c639350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ffd7c639350>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units = hidden_dim, state_is_tuple = True, activation = tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "animal-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "alleged-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "annoying-damage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 224.70346069335938\n",
      "[step: 1] loss: 139.9871826171875\n",
      "[step: 2] loss: 81.27420043945312\n",
      "[step: 3] loss: 46.35578155517578\n",
      "[step: 4] loss: 34.3590087890625\n",
      "[step: 5] loss: 41.753448486328125\n",
      "[step: 6] loss: 56.272911071777344\n",
      "[step: 7] loss: 62.926055908203125\n",
      "[step: 8] loss: 58.7321662902832\n",
      "[step: 9] loss: 48.842342376708984\n",
      "[step: 10] loss: 38.70280075073242\n",
      "[step: 11] loss: 31.340545654296875\n",
      "[step: 12] loss: 27.484882354736328\n",
      "[step: 13] loss: 26.51026153564453\n",
      "[step: 14] loss: 27.292993545532227\n",
      "[step: 15] loss: 28.72069549560547\n",
      "[step: 16] loss: 29.913257598876953\n",
      "[step: 17] loss: 30.284591674804688\n",
      "[step: 18] loss: 29.536731719970703\n",
      "[step: 19] loss: 27.631074905395508\n",
      "[step: 20] loss: 24.752761840820312\n",
      "[step: 21] loss: 21.269855499267578\n",
      "[step: 22] loss: 17.680204391479492\n",
      "[step: 23] loss: 14.531980514526367\n",
      "[step: 24] loss: 12.303170204162598\n",
      "[step: 25] loss: 11.240644454956055\n",
      "[step: 26] loss: 11.20505428314209\n",
      "[step: 27] loss: 11.635138511657715\n",
      "[step: 28] loss: 11.757881164550781\n",
      "[step: 29] loss: 11.016398429870605\n",
      "[step: 30] loss: 9.431859970092773\n",
      "[step: 31] loss: 7.588286399841309\n",
      "[step: 32] loss: 6.243715286254883\n",
      "[step: 33] loss: 5.867647647857666\n",
      "[step: 34] loss: 6.402631759643555\n",
      "[step: 35] loss: 7.351212978363037\n",
      "[step: 36] loss: 8.093826293945312\n",
      "[step: 37] loss: 8.236030578613281\n",
      "[step: 38] loss: 7.792781829833984\n",
      "[step: 39] loss: 7.125471115112305\n",
      "[step: 40] loss: 6.681396961212158\n",
      "[step: 41] loss: 6.686892509460449\n",
      "[step: 42] loss: 6.994001865386963\n",
      "[step: 43] loss: 7.2159223556518555\n",
      "[step: 44] loss: 7.05789852142334\n",
      "[step: 45] loss: 6.543803691864014\n",
      "[step: 46] loss: 5.946475028991699\n",
      "[step: 47] loss: 5.540068626403809\n",
      "[step: 48] loss: 5.418237686157227\n",
      "[step: 49] loss: 5.488427639007568\n",
      "[step: 50] loss: 5.583497047424316\n",
      "[step: 51] loss: 5.579636096954346\n",
      "[step: 52] loss: 5.452710151672363\n",
      "[step: 53] loss: 5.265113830566406\n",
      "[step: 54] loss: 5.110200881958008\n",
      "[step: 55] loss: 5.050928115844727\n",
      "[step: 56] loss: 5.084628105163574\n",
      "[step: 57] loss: 5.150778770446777\n",
      "[step: 58] loss: 5.174553394317627\n",
      "[step: 59] loss: 5.115954399108887\n",
      "[step: 60] loss: 4.99066686630249\n",
      "[step: 61] loss: 4.851039886474609\n",
      "[step: 62] loss: 4.745886325836182\n",
      "[step: 63] loss: 4.6899847984313965\n",
      "[step: 64] loss: 4.661685943603516\n",
      "[step: 65] loss: 4.624716758728027\n",
      "[step: 66] loss: 4.555855751037598\n",
      "[step: 67] loss: 4.459927558898926\n",
      "[step: 68] loss: 4.363987445831299\n",
      "[step: 69] loss: 4.296133995056152\n",
      "[step: 70] loss: 4.264469146728516\n",
      "[step: 71] loss: 4.252006530761719\n",
      "[step: 72] loss: 4.2316484451293945\n",
      "[step: 73] loss: 4.1884565353393555\n",
      "[step: 74] loss: 4.129682540893555\n",
      "[step: 75] loss: 4.074742317199707\n",
      "[step: 76] loss: 4.036294937133789\n",
      "[step: 77] loss: 4.010265350341797\n",
      "[step: 78] loss: 3.981947422027588\n",
      "[step: 79] loss: 3.9402289390563965\n",
      "[step: 80] loss: 3.88665509223938\n",
      "[step: 81] loss: 3.832569122314453\n",
      "[step: 82] loss: 3.7884042263031006\n",
      "[step: 83] loss: 3.755186080932617\n",
      "[step: 84] loss: 3.7253799438476562\n",
      "[step: 85] loss: 3.691199779510498\n",
      "[step: 86] loss: 3.6518101692199707\n",
      "[step: 87] loss: 3.612825393676758\n",
      "[step: 88] loss: 3.579751968383789\n",
      "[step: 89] loss: 3.552544593811035\n",
      "[step: 90] loss: 3.5262365341186523\n",
      "[step: 91] loss: 3.496079921722412\n",
      "[step: 92] loss: 3.461744546890259\n",
      "[step: 93] loss: 3.426882743835449\n",
      "[step: 94] loss: 3.395059585571289\n",
      "[step: 95] loss: 3.3663489818573\n",
      "[step: 96] loss: 3.3378498554229736\n",
      "[step: 97] loss: 3.30719256401062\n",
      "[step: 98] loss: 3.2750601768493652\n",
      "[step: 99] loss: 3.2441558837890625\n",
      "[step: 100] loss: 3.216109275817871\n",
      "[step: 101] loss: 3.189901113510132\n",
      "[step: 102] loss: 3.163313865661621\n",
      "[step: 103] loss: 3.1354353427886963\n",
      "[step: 104] loss: 3.107367753982544\n",
      "[step: 105] loss: 3.080582618713379\n",
      "[step: 106] loss: 3.0550856590270996\n",
      "[step: 107] loss: 3.029543161392212\n",
      "[step: 108] loss: 3.0030126571655273\n",
      "[step: 109] loss: 2.9760234355926514\n",
      "[step: 110] loss: 2.9497547149658203\n",
      "[step: 111] loss: 2.9245388507843018\n",
      "[step: 112] loss: 2.899613618850708\n",
      "[step: 113] loss: 2.8742520809173584\n",
      "[step: 114] loss: 2.8486711978912354\n",
      "[step: 115] loss: 2.823611259460449\n",
      "[step: 116] loss: 2.7992939949035645\n",
      "[step: 117] loss: 2.775174856185913\n",
      "[step: 118] loss: 2.7507288455963135\n",
      "[step: 119] loss: 2.7261061668395996\n",
      "[step: 120] loss: 2.7017858028411865\n",
      "[step: 121] loss: 2.6778371334075928\n",
      "[step: 122] loss: 2.6538658142089844\n",
      "[step: 123] loss: 2.6296281814575195\n",
      "[step: 124] loss: 2.6053671836853027\n",
      "[step: 125] loss: 2.5814049243927\n",
      "[step: 126] loss: 2.5576741695404053\n",
      "[step: 127] loss: 2.533874988555908\n",
      "[step: 128] loss: 2.509958267211914\n",
      "[step: 129] loss: 2.4861531257629395\n",
      "[step: 130] loss: 2.4625515937805176\n",
      "[step: 131] loss: 2.4389615058898926\n",
      "[step: 132] loss: 2.4152212142944336\n",
      "[step: 133] loss: 2.391441583633423\n",
      "[step: 134] loss: 2.367770195007324\n",
      "[step: 135] loss: 2.3441436290740967\n",
      "[step: 136] loss: 2.320427894592285\n",
      "[step: 137] loss: 2.296668291091919\n",
      "[step: 138] loss: 2.272991895675659\n",
      "[step: 139] loss: 2.2493748664855957\n",
      "[step: 140] loss: 2.2257003784179688\n",
      "[step: 141] loss: 2.2019808292388916\n",
      "[step: 142] loss: 2.178311586380005\n",
      "[step: 143] loss: 2.1546852588653564\n",
      "[step: 144] loss: 2.13102126121521\n",
      "[step: 145] loss: 2.1073362827301025\n",
      "[step: 146] loss: 2.0837011337280273\n",
      "[step: 147] loss: 2.0600998401641846\n",
      "[step: 148] loss: 2.0364797115325928\n",
      "[step: 149] loss: 2.012875556945801\n",
      "[step: 150] loss: 1.9893419742584229\n",
      "[step: 151] loss: 1.9658551216125488\n",
      "[step: 152] loss: 1.9423942565917969\n",
      "[step: 153] loss: 1.919004201889038\n",
      "[step: 154] loss: 1.8957016468048096\n",
      "[step: 155] loss: 1.8724555969238281\n",
      "[step: 156] loss: 1.8492788076400757\n",
      "[step: 157] loss: 1.8262145519256592\n",
      "[step: 158] loss: 1.8032597303390503\n",
      "[step: 159] loss: 1.7804105281829834\n",
      "[step: 160] loss: 1.7577049732208252\n",
      "[step: 161] loss: 1.7351531982421875\n",
      "[step: 162] loss: 1.7127435207366943\n",
      "[step: 163] loss: 1.690505862236023\n",
      "[step: 164] loss: 1.6684627532958984\n",
      "[step: 165] loss: 1.646609902381897\n",
      "[step: 166] loss: 1.6249679327011108\n",
      "[step: 167] loss: 1.6035568714141846\n",
      "[step: 168] loss: 1.5823702812194824\n",
      "[step: 169] loss: 1.5614283084869385\n",
      "[step: 170] loss: 1.54075026512146\n",
      "[step: 171] loss: 1.520336627960205\n",
      "[step: 172] loss: 1.5002026557922363\n",
      "[step: 173] loss: 1.4803587198257446\n",
      "[step: 174] loss: 1.4608008861541748\n",
      "[step: 175] loss: 1.4415438175201416\n",
      "[step: 176] loss: 1.4225945472717285\n",
      "[step: 177] loss: 1.4039514064788818\n",
      "[step: 178] loss: 1.3856244087219238\n",
      "[step: 179] loss: 1.3676095008850098\n",
      "[step: 180] loss: 1.349907636642456\n",
      "[step: 181] loss: 1.3325241804122925\n",
      "[step: 182] loss: 1.3154535293579102\n",
      "[step: 183] loss: 1.2986984252929688\n",
      "[step: 184] loss: 1.2822511196136475\n",
      "[step: 185] loss: 1.2661083936691284\n",
      "[step: 186] loss: 1.250269889831543\n",
      "[step: 187] loss: 1.234727144241333\n",
      "[step: 188] loss: 1.2194788455963135\n",
      "[step: 189] loss: 1.2045159339904785\n",
      "[step: 190] loss: 1.1898353099822998\n",
      "[step: 191] loss: 1.1754323244094849\n",
      "[step: 192] loss: 1.1613001823425293\n",
      "[step: 193] loss: 1.147435188293457\n",
      "[step: 194] loss: 1.133831262588501\n",
      "[step: 195] loss: 1.1204838752746582\n",
      "[step: 196] loss: 1.107387661933899\n",
      "[step: 197] loss: 1.0945405960083008\n",
      "[step: 198] loss: 1.0819355249404907\n",
      "[step: 199] loss: 1.069570541381836\n",
      "[step: 200] loss: 1.0574411153793335\n",
      "[step: 201] loss: 1.045543909072876\n",
      "[step: 202] loss: 1.033874273300171\n",
      "[step: 203] loss: 1.0224298238754272\n",
      "[step: 204] loss: 1.0112059116363525\n",
      "[step: 205] loss: 1.0002000331878662\n",
      "[step: 206] loss: 0.989408016204834\n",
      "[step: 207] loss: 0.9788263440132141\n",
      "[step: 208] loss: 0.9684518575668335\n",
      "[step: 209] loss: 0.9582803249359131\n",
      "[step: 210] loss: 0.948309063911438\n",
      "[step: 211] loss: 0.9385334253311157\n",
      "[step: 212] loss: 0.9289494752883911\n",
      "[step: 213] loss: 0.9195547699928284\n",
      "[step: 214] loss: 0.9103448390960693\n",
      "[step: 215] loss: 0.9013155698776245\n",
      "[step: 216] loss: 0.8924643397331238\n",
      "[step: 217] loss: 0.8837863206863403\n",
      "[step: 218] loss: 0.8752783536911011\n",
      "[step: 219] loss: 0.8669366240501404\n",
      "[step: 220] loss: 0.8587580919265747\n",
      "[step: 221] loss: 0.8507380485534668\n",
      "[step: 222] loss: 0.8428741097450256\n",
      "[step: 223] loss: 0.8351616859436035\n",
      "[step: 224] loss: 0.8275981545448303\n",
      "[step: 225] loss: 0.8201795220375061\n",
      "[step: 226] loss: 0.8129023313522339\n",
      "[step: 227] loss: 0.8057628870010376\n",
      "[step: 228] loss: 0.7987584471702576\n",
      "[step: 229] loss: 0.7918857336044312\n",
      "[step: 230] loss: 0.785140335559845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 231] loss: 0.7785203456878662\n",
      "[step: 232] loss: 0.7720214128494263\n",
      "[step: 233] loss: 0.7656413316726685\n",
      "[step: 234] loss: 0.7593761682510376\n",
      "[step: 235] loss: 0.7532229423522949\n",
      "[step: 236] loss: 0.7471786737442017\n",
      "[step: 237] loss: 0.7412406802177429\n",
      "[step: 238] loss: 0.7354055047035217\n",
      "[step: 239] loss: 0.7296704053878784\n",
      "[step: 240] loss: 0.7240325212478638\n",
      "[step: 241] loss: 0.7184889316558838\n",
      "[step: 242] loss: 0.7130365371704102\n",
      "[step: 243] loss: 0.707673192024231\n",
      "[step: 244] loss: 0.702396035194397\n",
      "[step: 245] loss: 0.6972018480300903\n",
      "[step: 246] loss: 0.6920887231826782\n",
      "[step: 247] loss: 0.6870535612106323\n",
      "[step: 248] loss: 0.6820936799049377\n",
      "[step: 249] loss: 0.6772074699401855\n",
      "[step: 250] loss: 0.6723917722702026\n",
      "[step: 251] loss: 0.6676449775695801\n",
      "[step: 252] loss: 0.6629643440246582\n",
      "[step: 253] loss: 0.6583477854728699\n",
      "[step: 254] loss: 0.6537930965423584\n",
      "[step: 255] loss: 0.6492988467216492\n",
      "[step: 256] loss: 0.6448625326156616\n",
      "[step: 257] loss: 0.6404824256896973\n",
      "[step: 258] loss: 0.6361572742462158\n",
      "[step: 259] loss: 0.6318843364715576\n",
      "[step: 260] loss: 0.6276636719703674\n",
      "[step: 261] loss: 0.6234920024871826\n",
      "[step: 262] loss: 0.6193687319755554\n",
      "[step: 263] loss: 0.6152925491333008\n",
      "[step: 264] loss: 0.6112621426582336\n",
      "[step: 265] loss: 0.6072761416435242\n",
      "[step: 266] loss: 0.6033337116241455\n",
      "[step: 267] loss: 0.5994335412979126\n",
      "[step: 268] loss: 0.5955746173858643\n",
      "[step: 269] loss: 0.5917563438415527\n",
      "[step: 270] loss: 0.5879775285720825\n",
      "[step: 271] loss: 0.5842375159263611\n",
      "[step: 272] loss: 0.5805357694625854\n",
      "[step: 273] loss: 0.5768718123435974\n",
      "[step: 274] loss: 0.573244571685791\n",
      "[step: 275] loss: 0.569654107093811\n",
      "[step: 276] loss: 0.5660995244979858\n",
      "[step: 277] loss: 0.5625806450843811\n",
      "[step: 278] loss: 0.559097170829773\n",
      "[step: 279] loss: 0.5556485652923584\n",
      "[step: 280] loss: 0.5522348284721375\n",
      "[step: 281] loss: 0.5488553047180176\n",
      "[step: 282] loss: 0.5455106496810913\n",
      "[step: 283] loss: 0.5422006845474243\n",
      "[step: 284] loss: 0.5389242768287659\n",
      "[step: 285] loss: 0.535682737827301\n",
      "[step: 286] loss: 0.5324754118919373\n",
      "[step: 287] loss: 0.5293020606040955\n",
      "[step: 288] loss: 0.526163637638092\n",
      "[step: 289] loss: 0.5230593681335449\n",
      "[step: 290] loss: 0.5199896693229675\n",
      "[step: 291] loss: 0.5169553756713867\n",
      "[step: 292] loss: 0.513955295085907\n",
      "[step: 293] loss: 0.5109909772872925\n",
      "[step: 294] loss: 0.5080621838569641\n",
      "[step: 295] loss: 0.5051687359809875\n",
      "[step: 296] loss: 0.5023114085197449\n",
      "[step: 297] loss: 0.4994901120662689\n",
      "[step: 298] loss: 0.4967058300971985\n",
      "[step: 299] loss: 0.4939577579498291\n",
      "[step: 300] loss: 0.4912473261356354\n",
      "[step: 301] loss: 0.48857372999191284\n",
      "[step: 302] loss: 0.4859382212162018\n",
      "[step: 303] loss: 0.4833407998085022\n",
      "[step: 304] loss: 0.48078155517578125\n",
      "[step: 305] loss: 0.47826093435287476\n",
      "[step: 306] loss: 0.47577911615371704\n",
      "[step: 307] loss: 0.47333669662475586\n",
      "[step: 308] loss: 0.470933735370636\n",
      "[step: 309] loss: 0.4685708284378052\n",
      "[step: 310] loss: 0.4662476181983948\n",
      "[step: 311] loss: 0.4639645516872406\n",
      "[step: 312] loss: 0.4617219567298889\n",
      "[step: 313] loss: 0.4595201015472412\n",
      "[step: 314] loss: 0.4573594927787781\n",
      "[step: 315] loss: 0.4552396535873413\n",
      "[step: 316] loss: 0.4531610608100891\n",
      "[step: 317] loss: 0.45112356543540955\n",
      "[step: 318] loss: 0.44912827014923096\n",
      "[step: 319] loss: 0.44717538356781006\n",
      "[step: 320] loss: 0.44526737928390503\n",
      "[step: 321] loss: 0.44341015815734863\n",
      "[step: 322] loss: 0.4416224956512451\n",
      "[step: 323] loss: 0.4399600923061371\n",
      "[step: 324] loss: 0.43859678506851196\n",
      "[step: 325] loss: 0.43809187412261963\n",
      "[step: 326] loss: 0.4402407705783844\n",
      "[step: 327] loss: 0.45113393664360046\n",
      "[step: 328] loss: 0.48977887630462646\n",
      "[step: 329] loss: 0.6186794638633728\n",
      "[step: 330] loss: 0.9495772123336792\n",
      "[step: 331] loss: 1.648911476135254\n",
      "[step: 332] loss: 1.7849979400634766\n",
      "[step: 333] loss: 1.1904022693634033\n",
      "[step: 334] loss: 0.4321397542953491\n",
      "[step: 335] loss: 1.0109541416168213\n",
      "[step: 336] loss: 1.2135334014892578\n",
      "[step: 337] loss: 0.4527119994163513\n",
      "[step: 338] loss: 0.9418264627456665\n",
      "[step: 339] loss: 0.9491645097732544\n",
      "[step: 340] loss: 0.47251591086387634\n",
      "[step: 341] loss: 1.0024216175079346\n",
      "[step: 342] loss: 0.6226868033409119\n",
      "[step: 343] loss: 0.6302083730697632\n",
      "[step: 344] loss: 0.8096820116043091\n",
      "[step: 345] loss: 0.45325401425361633\n",
      "[step: 346] loss: 0.7312631607055664\n",
      "[step: 347] loss: 0.5076373815536499\n",
      "[step: 348] loss: 0.5711098313331604\n",
      "[step: 349] loss: 0.6119294166564941\n",
      "[step: 350] loss: 0.45893555879592896\n",
      "[step: 351] loss: 0.6322922706604004\n",
      "[step: 352] loss: 0.4601054787635803\n",
      "[step: 353] loss: 0.5390301942825317\n",
      "[step: 354] loss: 0.5216147303581238\n",
      "[step: 355] loss: 0.44905251264572144\n",
      "[step: 356] loss: 0.5414282083511353\n",
      "[step: 357] loss: 0.43749961256980896\n",
      "[step: 358] loss: 0.495114266872406\n",
      "[step: 359] loss: 0.4804089665412903\n",
      "[step: 360] loss: 0.4348661005496979\n",
      "[step: 361] loss: 0.49540507793426514\n",
      "[step: 362] loss: 0.4312742352485657\n",
      "[step: 363] loss: 0.45169633626937866\n",
      "[step: 364] loss: 0.45967891812324524\n",
      "[step: 365] loss: 0.41665637493133545\n",
      "[step: 366] loss: 0.45279839634895325\n",
      "[step: 367] loss: 0.4275834560394287\n",
      "[step: 368] loss: 0.41966748237609863\n",
      "[step: 369] loss: 0.44101157784461975\n",
      "[step: 370] loss: 0.41184020042419434\n",
      "[step: 371] loss: 0.4229740798473358\n",
      "[step: 372] loss: 0.42712894082069397\n",
      "[step: 373] loss: 0.4064929187297821\n",
      "[step: 374] loss: 0.4215623736381531\n",
      "[step: 375] loss: 0.4164917469024658\n",
      "[step: 376] loss: 0.40440139174461365\n",
      "[step: 377] loss: 0.41711360216140747\n",
      "[step: 378] loss: 0.40955203771591187\n",
      "[step: 379] loss: 0.4024026393890381\n",
      "[step: 380] loss: 0.41192904114723206\n",
      "[step: 381] loss: 0.40520480275154114\n",
      "[step: 382] loss: 0.4000541865825653\n",
      "[step: 383] loss: 0.40713179111480713\n",
      "[step: 384] loss: 0.4023963212966919\n",
      "[step: 385] loss: 0.39771148562431335\n",
      "[step: 386] loss: 0.4028897285461426\n",
      "[step: 387] loss: 0.400346040725708\n",
      "[step: 388] loss: 0.3957057595252991\n",
      "[step: 389] loss: 0.39909934997558594\n",
      "[step: 390] loss: 0.3985062837600708\n",
      "[step: 391] loss: 0.39417409896850586\n",
      "[step: 392] loss: 0.3957492709159851\n",
      "[step: 393] loss: 0.39654189348220825\n",
      "[step: 394] loss: 0.3930682837963104\n",
      "[step: 395] loss: 0.39298683404922485\n",
      "[step: 396] loss: 0.39437270164489746\n",
      "[step: 397] loss: 0.3921948969364166\n",
      "[step: 398] loss: 0.3909517228603363\n",
      "[step: 399] loss: 0.39212068915367126\n",
      "[step: 400] loss: 0.391282320022583\n",
      "[step: 401] loss: 0.3896115720272064\n",
      "[step: 402] loss: 0.39002272486686707\n",
      "[step: 403] loss: 0.39012300968170166\n",
      "[step: 404] loss: 0.3887307643890381\n",
      "[step: 405] loss: 0.38830244541168213\n",
      "[step: 406] loss: 0.3886834979057312\n",
      "[step: 407] loss: 0.3879625201225281\n",
      "[step: 408] loss: 0.3870564103126526\n",
      "[step: 409] loss: 0.3871399164199829\n",
      "[step: 410] loss: 0.38701334595680237\n",
      "[step: 411] loss: 0.38617396354675293\n",
      "[step: 412] loss: 0.3857685327529907\n",
      "[step: 413] loss: 0.3858087360858917\n",
      "[step: 414] loss: 0.38537219166755676\n",
      "[step: 415] loss: 0.3847278952598572\n",
      "[step: 416] loss: 0.3845333456993103\n",
      "[step: 417] loss: 0.3844032883644104\n",
      "[step: 418] loss: 0.3839045763015747\n",
      "[step: 419] loss: 0.38344040513038635\n",
      "[step: 420] loss: 0.3832753002643585\n",
      "[step: 421] loss: 0.38303515315055847\n",
      "[step: 422] loss: 0.38257133960723877\n",
      "[step: 423] loss: 0.38220787048339844\n",
      "[step: 424] loss: 0.38201650977134705\n",
      "[step: 425] loss: 0.38173186779022217\n",
      "[step: 426] loss: 0.3813225030899048\n",
      "[step: 427] loss: 0.38100308179855347\n",
      "[step: 428] loss: 0.380784809589386\n",
      "[step: 429] loss: 0.38049477338790894\n",
      "[step: 430] loss: 0.3801300823688507\n",
      "[step: 431] loss: 0.3798278570175171\n",
      "[step: 432] loss: 0.37959134578704834\n",
      "[step: 433] loss: 0.3793099820613861\n",
      "[step: 434] loss: 0.3789779841899872\n",
      "[step: 435] loss: 0.3786826431751251\n",
      "[step: 436] loss: 0.37843430042266846\n",
      "[step: 437] loss: 0.3781648576259613\n",
      "[step: 438] loss: 0.37785768508911133\n",
      "[step: 439] loss: 0.37756577134132385\n",
      "[step: 440] loss: 0.37730899453163147\n",
      "[step: 441] loss: 0.3770492374897003\n",
      "[step: 442] loss: 0.37676289677619934\n",
      "[step: 443] loss: 0.3764764964580536\n",
      "[step: 444] loss: 0.37621358036994934\n",
      "[step: 445] loss: 0.3759590685367584\n",
      "[step: 446] loss: 0.3756898045539856\n",
      "[step: 447] loss: 0.37541234493255615\n",
      "[step: 448] loss: 0.3751460909843445\n",
      "[step: 449] loss: 0.374891996383667\n",
      "[step: 450] loss: 0.374634712934494\n",
      "[step: 451] loss: 0.3743686079978943\n",
      "[step: 452] loss: 0.374103307723999\n",
      "[step: 453] loss: 0.3738476037979126\n",
      "[step: 454] loss: 0.37359628081321716\n",
      "[step: 455] loss: 0.37334126234054565\n",
      "[step: 456] loss: 0.3730824887752533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 457] loss: 0.37282639741897583\n",
      "[step: 458] loss: 0.3725762963294983\n",
      "[step: 459] loss: 0.3723277747631073\n",
      "[step: 460] loss: 0.3720777630805969\n",
      "[step: 461] loss: 0.3718259632587433\n",
      "[step: 462] loss: 0.3715764284133911\n",
      "[step: 463] loss: 0.3713306784629822\n",
      "[step: 464] loss: 0.3710862994194031\n",
      "[step: 465] loss: 0.3708415627479553\n",
      "[step: 466] loss: 0.37059566378593445\n",
      "[step: 467] loss: 0.3703514635562897\n",
      "[step: 468] loss: 0.3701093792915344\n",
      "[step: 469] loss: 0.36986926198005676\n",
      "[step: 470] loss: 0.36962974071502686\n",
      "[step: 471] loss: 0.36938953399658203\n",
      "[step: 472] loss: 0.36914998292922974\n",
      "[step: 473] loss: 0.3689119815826416\n",
      "[step: 474] loss: 0.3686758279800415\n",
      "[step: 475] loss: 0.3684401214122772\n",
      "[step: 476] loss: 0.36820513010025024\n",
      "[step: 477] loss: 0.36797022819519043\n",
      "[step: 478] loss: 0.3677366077899933\n",
      "[step: 479] loss: 0.367503821849823\n",
      "[step: 480] loss: 0.3672724664211273\n",
      "[step: 481] loss: 0.3670416474342346\n",
      "[step: 482] loss: 0.3668115735054016\n",
      "[step: 483] loss: 0.3665822446346283\n",
      "[step: 484] loss: 0.36635324358940125\n",
      "[step: 485] loss: 0.3661251664161682\n",
      "[step: 486] loss: 0.365898460149765\n",
      "[step: 487] loss: 0.36567237973213196\n",
      "[step: 488] loss: 0.36544710397720337\n",
      "[step: 489] loss: 0.3652224838733673\n",
      "[step: 490] loss: 0.36499834060668945\n",
      "[step: 491] loss: 0.36477527022361755\n",
      "[step: 492] loss: 0.36455267667770386\n",
      "[step: 493] loss: 0.364330917596817\n",
      "[step: 494] loss: 0.36411017179489136\n",
      "[step: 495] loss: 0.36389005184173584\n",
      "[step: 496] loss: 0.3636707067489624\n",
      "[step: 497] loss: 0.36345183849334717\n",
      "[step: 498] loss: 0.36323389410972595\n",
      "[step: 499] loss: 0.36301660537719727\n",
      "[step: 500] loss: 0.3627997636795044\n",
      "[step: 501] loss: 0.3625839352607727\n",
      "[step: 502] loss: 0.362368643283844\n",
      "[step: 503] loss: 0.3621543347835541\n",
      "[step: 504] loss: 0.36194029450416565\n",
      "[step: 505] loss: 0.36172735691070557\n",
      "[step: 506] loss: 0.3615148365497589\n",
      "[step: 507] loss: 0.36130309104919434\n",
      "[step: 508] loss: 0.3610920011997223\n",
      "[step: 509] loss: 0.360881507396698\n",
      "[step: 510] loss: 0.3606717586517334\n",
      "[step: 511] loss: 0.3604627251625061\n",
      "[step: 512] loss: 0.3602544069290161\n",
      "[step: 513] loss: 0.36004674434661865\n",
      "[step: 514] loss: 0.3598395586013794\n",
      "[step: 515] loss: 0.3596331775188446\n",
      "[step: 516] loss: 0.3594273030757904\n",
      "[step: 517] loss: 0.3592222034931183\n",
      "[step: 518] loss: 0.35901767015457153\n",
      "[step: 519] loss: 0.35881364345550537\n",
      "[step: 520] loss: 0.358610600233078\n",
      "[step: 521] loss: 0.3584078550338745\n",
      "[step: 522] loss: 0.3582059442996979\n",
      "[step: 523] loss: 0.3580048084259033\n",
      "[step: 524] loss: 0.3578040599822998\n",
      "[step: 525] loss: 0.3576040267944336\n",
      "[step: 526] loss: 0.35740476846694946\n",
      "[step: 527] loss: 0.3572057783603668\n",
      "[step: 528] loss: 0.35700786113739014\n",
      "[step: 529] loss: 0.35681039094924927\n",
      "[step: 530] loss: 0.35661381483078003\n",
      "[step: 531] loss: 0.35641801357269287\n",
      "[step: 532] loss: 0.356223464012146\n",
      "[step: 533] loss: 0.35603004693984985\n",
      "[step: 534] loss: 0.3558388948440552\n",
      "[step: 535] loss: 0.35565102100372314\n",
      "[step: 536] loss: 0.3554689288139343\n",
      "[step: 537] loss: 0.35529688000679016\n",
      "[step: 538] loss: 0.35514456033706665\n",
      "[step: 539] loss: 0.35502997040748596\n",
      "[step: 540] loss: 0.354990690946579\n",
      "[step: 541] loss: 0.3551027774810791\n",
      "[step: 542] loss: 0.3555324077606201\n",
      "[step: 543] loss: 0.3566114008426666\n",
      "[step: 544] loss: 0.3591156601905823\n",
      "[step: 545] loss: 0.36455458402633667\n",
      "[step: 546] loss: 0.3767763376235962\n",
      "[step: 547] loss: 0.4025627076625824\n",
      "[step: 548] loss: 0.4615946114063263\n",
      "[step: 549] loss: 0.5781280994415283\n",
      "[step: 550] loss: 0.8403857946395874\n",
      "[step: 551] loss: 1.214526653289795\n",
      "[step: 552] loss: 1.8403712511062622\n",
      "[step: 553] loss: 1.7175912857055664\n",
      "[step: 554] loss: 1.148521900177002\n",
      "[step: 555] loss: 0.3931136727333069\n",
      "[step: 556] loss: 0.7090240716934204\n",
      "[step: 557] loss: 1.136399745941162\n",
      "[step: 558] loss: 0.5348844528198242\n",
      "[step: 559] loss: 0.5088386535644531\n",
      "[step: 560] loss: 0.8803704380989075\n",
      "[step: 561] loss: 0.4801352024078369\n",
      "[step: 562] loss: 0.5122548341751099\n",
      "[step: 563] loss: 0.7276431918144226\n",
      "[step: 564] loss: 0.40869373083114624\n",
      "[step: 565] loss: 0.5483645796775818\n",
      "[step: 566] loss: 0.6131502389907837\n",
      "[step: 567] loss: 0.38174134492874146\n",
      "[step: 568] loss: 0.5690953731536865\n",
      "[step: 569] loss: 0.5227060914039612\n",
      "[step: 570] loss: 0.3865363597869873\n",
      "[step: 571] loss: 0.5575189590454102\n",
      "[step: 572] loss: 0.4614936411380768\n",
      "[step: 573] loss: 0.39674830436706543\n",
      "[step: 574] loss: 0.5253866314888\n",
      "[step: 575] loss: 0.42292720079421997\n",
      "[step: 576] loss: 0.39909598231315613\n",
      "[step: 577] loss: 0.48681342601776123\n",
      "[step: 578] loss: 0.3973771333694458\n",
      "[step: 579] loss: 0.39463746547698975\n",
      "[step: 580] loss: 0.45164209604263306\n",
      "[step: 581] loss: 0.38060396909713745\n",
      "[step: 582] loss: 0.38670849800109863\n",
      "[step: 583] loss: 0.42388325929641724\n",
      "[step: 584] loss: 0.3707217574119568\n",
      "[step: 585] loss: 0.3782522976398468\n",
      "[step: 586] loss: 0.4042647182941437\n",
      "[step: 587] loss: 0.36564183235168457\n",
      "[step: 588] loss: 0.3701092600822449\n",
      "[step: 589] loss: 0.3904511630535126\n",
      "[step: 590] loss: 0.363179087638855\n",
      "[step: 591] loss: 0.3625369668006897\n",
      "[step: 592] loss: 0.3801061511039734\n",
      "[step: 593] loss: 0.36243224143981934\n",
      "[step: 594] loss: 0.3560152053833008\n",
      "[step: 595] loss: 0.3711143136024475\n",
      "[step: 596] loss: 0.3629222512245178\n",
      "[step: 597] loss: 0.35201212763786316\n",
      "[step: 598] loss: 0.3624340295791626\n",
      "[step: 599] loss: 0.3632105886936188\n",
      "[step: 600] loss: 0.3516538143157959\n",
      "[step: 601] loss: 0.35458609461784363\n",
      "[step: 602] loss: 0.3608478605747223\n",
      "[step: 603] loss: 0.353895366191864\n",
      "[step: 604] loss: 0.34990882873535156\n",
      "[step: 605] loss: 0.3554230034351349\n",
      "[step: 606] loss: 0.35531890392303467\n",
      "[step: 607] loss: 0.3495769500732422\n",
      "[step: 608] loss: 0.34999391436576843\n",
      "[step: 609] loss: 0.3534216284751892\n",
      "[step: 610] loss: 0.35124844312667847\n",
      "[step: 611] loss: 0.34795165061950684\n",
      "[step: 612] loss: 0.34957465529441833\n",
      "[step: 613] loss: 0.3511471152305603\n",
      "[step: 614] loss: 0.34876543283462524\n",
      "[step: 615] loss: 0.3472297787666321\n",
      "[step: 616] loss: 0.3487564027309418\n",
      "[step: 617] loss: 0.34918510913848877\n",
      "[step: 618] loss: 0.34735000133514404\n",
      "[step: 619] loss: 0.34666213393211365\n",
      "[step: 620] loss: 0.3477269411087036\n",
      "[step: 621] loss: 0.34774231910705566\n",
      "[step: 622] loss: 0.3464142680168152\n",
      "[step: 623] loss: 0.34603452682495117\n",
      "[step: 624] loss: 0.3467080891132355\n",
      "[step: 625] loss: 0.3466082513332367\n",
      "[step: 626] loss: 0.34568631649017334\n",
      "[step: 627] loss: 0.3453785181045532\n",
      "[step: 628] loss: 0.3457692861557007\n",
      "[step: 629] loss: 0.3457110822200775\n",
      "[step: 630] loss: 0.3450626730918884\n",
      "[step: 631] loss: 0.34474068880081177\n",
      "[step: 632] loss: 0.3449275493621826\n",
      "[step: 633] loss: 0.34491196274757385\n",
      "[step: 634] loss: 0.3444685935974121\n",
      "[step: 635] loss: 0.3441277742385864\n",
      "[step: 636] loss: 0.34415310621261597\n",
      "[step: 637] loss: 0.3441617786884308\n",
      "[step: 638] loss: 0.34387534856796265\n",
      "[step: 639] loss: 0.34354913234710693\n",
      "[step: 640] loss: 0.3434549868106842\n",
      "[step: 641] loss: 0.3434486985206604\n",
      "[step: 642] loss: 0.34328073263168335\n",
      "[step: 643] loss: 0.3430032730102539\n",
      "[step: 644] loss: 0.3428310751914978\n",
      "[step: 645] loss: 0.3427778482437134\n",
      "[step: 646] loss: 0.3426773250102997\n",
      "[step: 647] loss: 0.34246909618377686\n",
      "[step: 648] loss: 0.34226560592651367\n",
      "[step: 649] loss: 0.34215205907821655\n",
      "[step: 650] loss: 0.34206849336624146\n",
      "[step: 651] loss: 0.34192466735839844\n",
      "[step: 652] loss: 0.3417358994483948\n",
      "[step: 653] loss: 0.34157854318618774\n",
      "[step: 654] loss: 0.3414718806743622\n",
      "[step: 655] loss: 0.3413633704185486\n",
      "[step: 656] loss: 0.3412143588066101\n",
      "[step: 657] loss: 0.3410494029521942\n",
      "[step: 658] loss: 0.3409102261066437\n",
      "[step: 659] loss: 0.3407970666885376\n",
      "[step: 660] loss: 0.3406779170036316\n",
      "[step: 661] loss: 0.3405352532863617\n",
      "[step: 662] loss: 0.3403856158256531\n",
      "[step: 663] loss: 0.3402511477470398\n",
      "[step: 664] loss: 0.3401322662830353\n",
      "[step: 665] loss: 0.3400103449821472\n",
      "[step: 666] loss: 0.3398752808570862\n",
      "[step: 667] loss: 0.3397345542907715\n",
      "[step: 668] loss: 0.33960163593292236\n",
      "[step: 669] loss: 0.3394785523414612\n",
      "[step: 670] loss: 0.3393571972846985\n",
      "[step: 671] loss: 0.33922845125198364\n",
      "[step: 672] loss: 0.33909448981285095\n",
      "[step: 673] loss: 0.3389628827571869\n",
      "[step: 674] loss: 0.33883750438690186\n",
      "[step: 675] loss: 0.3387153744697571\n",
      "[step: 676] loss: 0.33859115839004517\n",
      "[step: 677] loss: 0.33846333622932434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 678] loss: 0.33833423256874084\n",
      "[step: 679] loss: 0.33820757269859314\n",
      "[step: 680] loss: 0.33808448910713196\n",
      "[step: 681] loss: 0.33796238899230957\n",
      "[step: 682] loss: 0.3378390073776245\n",
      "[step: 683] loss: 0.33771395683288574\n",
      "[step: 684] loss: 0.33758869767189026\n",
      "[step: 685] loss: 0.3374648690223694\n",
      "[step: 686] loss: 0.3373429775238037\n",
      "[step: 687] loss: 0.3372216522693634\n",
      "[step: 688] loss: 0.3371003270149231\n",
      "[step: 689] loss: 0.3369781970977783\n",
      "[step: 690] loss: 0.33685585856437683\n",
      "[step: 691] loss: 0.33673393726348877\n",
      "[step: 692] loss: 0.3366132378578186\n",
      "[step: 693] loss: 0.3364931046962738\n",
      "[step: 694] loss: 0.3363732695579529\n",
      "[step: 695] loss: 0.3362538814544678\n",
      "[step: 696] loss: 0.3361339569091797\n",
      "[step: 697] loss: 0.33601418137550354\n",
      "[step: 698] loss: 0.3358948230743408\n",
      "[step: 699] loss: 0.3357761800289154\n",
      "[step: 700] loss: 0.33565813302993774\n",
      "[step: 701] loss: 0.33554017543792725\n",
      "[step: 702] loss: 0.3354226052761078\n",
      "[step: 703] loss: 0.33530500531196594\n",
      "[step: 704] loss: 0.3351878523826599\n",
      "[step: 705] loss: 0.3350708484649658\n",
      "[step: 706] loss: 0.33495402336120605\n",
      "[step: 707] loss: 0.33483749628067017\n",
      "[step: 708] loss: 0.3347215950489044\n",
      "[step: 709] loss: 0.3346056044101715\n",
      "[step: 710] loss: 0.33449041843414307\n",
      "[step: 711] loss: 0.3343753516674042\n",
      "[step: 712] loss: 0.3342602252960205\n",
      "[step: 713] loss: 0.33414578437805176\n",
      "[step: 714] loss: 0.33403104543685913\n",
      "[step: 715] loss: 0.33391711115837097\n",
      "[step: 716] loss: 0.33380308747291565\n",
      "[step: 717] loss: 0.3336893320083618\n",
      "[step: 718] loss: 0.3335760831832886\n",
      "[step: 719] loss: 0.33346307277679443\n",
      "[step: 720] loss: 0.33335041999816895\n",
      "[step: 721] loss: 0.3332378566265106\n",
      "[step: 722] loss: 0.33312562108039856\n",
      "[step: 723] loss: 0.33301353454589844\n",
      "[step: 724] loss: 0.33290213346481323\n",
      "[step: 725] loss: 0.3327904939651489\n",
      "[step: 726] loss: 0.33267951011657715\n",
      "[step: 727] loss: 0.3325687050819397\n",
      "[step: 728] loss: 0.3324577808380127\n",
      "[step: 729] loss: 0.33234715461730957\n",
      "[step: 730] loss: 0.3322371244430542\n",
      "[step: 731] loss: 0.3321274518966675\n",
      "[step: 732] loss: 0.3320176899433136\n",
      "[step: 733] loss: 0.3319081962108612\n",
      "[step: 734] loss: 0.33179914951324463\n",
      "[step: 735] loss: 0.33169031143188477\n",
      "[step: 736] loss: 0.331581711769104\n",
      "[step: 737] loss: 0.33147329092025757\n",
      "[step: 738] loss: 0.3313653767108917\n",
      "[step: 739] loss: 0.3312578797340393\n",
      "[step: 740] loss: 0.33115068078041077\n",
      "[step: 741] loss: 0.33104389905929565\n",
      "[step: 742] loss: 0.330937922000885\n",
      "[step: 743] loss: 0.33083295822143555\n",
      "[step: 744] loss: 0.3307296633720398\n",
      "[step: 745] loss: 0.3306284248828888\n",
      "[step: 746] loss: 0.33053189516067505\n",
      "[step: 747] loss: 0.33044251799583435\n",
      "[step: 748] loss: 0.3303667902946472\n",
      "[step: 749] loss: 0.3303159475326538\n",
      "[step: 750] loss: 0.3303114175796509\n",
      "[step: 751] loss: 0.3303965926170349\n",
      "[step: 752] loss: 0.3306528925895691\n",
      "[step: 753] loss: 0.33125361800193787\n",
      "[step: 754] loss: 0.33252018690109253\n",
      "[step: 755] loss: 0.3351876735687256\n",
      "[step: 756] loss: 0.34056851267814636\n",
      "[step: 757] loss: 0.35196444392204285\n",
      "[step: 758] loss: 0.3746453821659088\n",
      "[step: 759] loss: 0.42404329776763916\n",
      "[step: 760] loss: 0.5174909830093384\n",
      "[step: 761] loss: 0.7226303219795227\n",
      "[step: 762] loss: 1.0322771072387695\n",
      "[step: 763] loss: 1.6186846494674683\n",
      "[step: 764] loss: 1.8294719457626343\n",
      "[step: 765] loss: 1.7806479930877686\n",
      "[step: 766] loss: 0.7374424338340759\n",
      "[step: 767] loss: 0.3743916153907776\n",
      "[step: 768] loss: 0.8995209336280823\n",
      "[step: 769] loss: 0.8699612617492676\n",
      "[step: 770] loss: 0.41311919689178467\n",
      "[step: 771] loss: 0.5217972993850708\n",
      "[step: 772] loss: 0.7204856872558594\n",
      "[step: 773] loss: 0.4497891366481781\n",
      "[step: 774] loss: 0.42684057354927063\n",
      "[step: 775] loss: 0.608932375907898\n",
      "[step: 776] loss: 0.43474316596984863\n",
      "[step: 777] loss: 0.39358240365982056\n",
      "[step: 778] loss: 0.5398150682449341\n",
      "[step: 779] loss: 0.41905921697616577\n",
      "[step: 780] loss: 0.37491273880004883\n",
      "[step: 781] loss: 0.4938881993293762\n",
      "[step: 782] loss: 0.41598156094551086\n",
      "[step: 783] loss: 0.3629753589630127\n",
      "[step: 784] loss: 0.4578533172607422\n",
      "[step: 785] loss: 0.41478925943374634\n",
      "[step: 786] loss: 0.3532346487045288\n",
      "[step: 787] loss: 0.42385947704315186\n",
      "[step: 788] loss: 0.4072558581829071\n",
      "[step: 789] loss: 0.34630224108695984\n",
      "[step: 790] loss: 0.3949604034423828\n",
      "[step: 791] loss: 0.39600375294685364\n",
      "[step: 792] loss: 0.3420723080635071\n",
      "[step: 793] loss: 0.3725782334804535\n",
      "[step: 794] loss: 0.38565605878829956\n",
      "[step: 795] loss: 0.3418489694595337\n",
      "[step: 796] loss: 0.35602402687072754\n",
      "[step: 797] loss: 0.37549710273742676\n",
      "[step: 798] loss: 0.34348246455192566\n",
      "[step: 799] loss: 0.3433533310890198\n",
      "[step: 800] loss: 0.3642226755619049\n",
      "[step: 801] loss: 0.34598368406295776\n",
      "[step: 802] loss: 0.3345556855201721\n",
      "[step: 803] loss: 0.35075145959854126\n",
      "[step: 804] loss: 0.34791597723960876\n",
      "[step: 805] loss: 0.33250144124031067\n",
      "[step: 806] loss: 0.33747339248657227\n",
      "[step: 807] loss: 0.34571221470832825\n",
      "[step: 808] loss: 0.33713069558143616\n",
      "[step: 809] loss: 0.3304842710494995\n",
      "[step: 810] loss: 0.3374786376953125\n",
      "[step: 811] loss: 0.3402848243713379\n",
      "[step: 812] loss: 0.3326377868652344\n",
      "[step: 813] loss: 0.33014732599258423\n",
      "[step: 814] loss: 0.3353671431541443\n",
      "[step: 815] loss: 0.3358049988746643\n",
      "[step: 816] loss: 0.3305625319480896\n",
      "[step: 817] loss: 0.3292914628982544\n",
      "[step: 818] loss: 0.3327040672302246\n",
      "[step: 819] loss: 0.333032488822937\n",
      "[step: 820] loss: 0.3295155465602875\n",
      "[step: 821] loss: 0.3285403847694397\n",
      "[step: 822] loss: 0.3307827413082123\n",
      "[step: 823] loss: 0.3311881422996521\n",
      "[step: 824] loss: 0.32896095514297485\n",
      "[step: 825] loss: 0.327961802482605\n",
      "[step: 826] loss: 0.32925963401794434\n",
      "[step: 827] loss: 0.32986190915107727\n",
      "[step: 828] loss: 0.3285062909126282\n",
      "[step: 829] loss: 0.327450156211853\n",
      "[step: 830] loss: 0.32801124453544617\n",
      "[step: 831] loss: 0.32865822315216064\n",
      "[step: 832] loss: 0.3280579447746277\n",
      "[step: 833] loss: 0.3271067142486572\n",
      "[step: 834] loss: 0.3271002769470215\n",
      "[step: 835] loss: 0.32762962579727173\n",
      "[step: 836] loss: 0.3275538682937622\n",
      "[step: 837] loss: 0.32688722014427185\n",
      "[step: 838] loss: 0.32651829719543457\n",
      "[step: 839] loss: 0.32673120498657227\n",
      "[step: 840] loss: 0.3269064128398895\n",
      "[step: 841] loss: 0.32659998536109924\n",
      "[step: 842] loss: 0.3261553645133972\n",
      "[step: 843] loss: 0.32604679465293884\n",
      "[step: 844] loss: 0.32618728280067444\n",
      "[step: 845] loss: 0.32616397738456726\n",
      "[step: 846] loss: 0.32587531208992004\n",
      "[step: 847] loss: 0.3256125748157501\n",
      "[step: 848] loss: 0.3255769610404968\n",
      "[step: 849] loss: 0.3256263732910156\n",
      "[step: 850] loss: 0.3255360722541809\n",
      "[step: 851] loss: 0.3253124952316284\n",
      "[step: 852] loss: 0.32513904571533203\n",
      "[step: 853] loss: 0.3250983953475952\n",
      "[step: 854] loss: 0.32508769631385803\n",
      "[step: 855] loss: 0.32498911023139954\n",
      "[step: 856] loss: 0.324818879365921\n",
      "[step: 857] loss: 0.32468199729919434\n",
      "[step: 858] loss: 0.3246224820613861\n",
      "[step: 859] loss: 0.32458388805389404\n",
      "[step: 860] loss: 0.32449716329574585\n",
      "[step: 861] loss: 0.324365496635437\n",
      "[step: 862] loss: 0.3242441713809967\n",
      "[step: 863] loss: 0.32416683435440063\n",
      "[step: 864] loss: 0.3241107165813446\n",
      "[step: 865] loss: 0.3240349292755127\n",
      "[step: 866] loss: 0.3239285945892334\n",
      "[step: 867] loss: 0.3238171339035034\n",
      "[step: 868] loss: 0.3237265348434448\n",
      "[step: 869] loss: 0.32365596294403076\n",
      "[step: 870] loss: 0.323584645986557\n",
      "[step: 871] loss: 0.323497474193573\n",
      "[step: 872] loss: 0.3233978748321533\n",
      "[step: 873] loss: 0.32330283522605896\n",
      "[step: 874] loss: 0.32322025299072266\n",
      "[step: 875] loss: 0.32314586639404297\n",
      "[step: 876] loss: 0.3230683207511902\n",
      "[step: 877] loss: 0.32298189401626587\n",
      "[step: 878] loss: 0.32289010286331177\n",
      "[step: 879] loss: 0.32280147075653076\n",
      "[step: 880] loss: 0.3227195143699646\n",
      "[step: 881] loss: 0.3226417005062103\n",
      "[step: 882] loss: 0.3225628435611725\n",
      "[step: 883] loss: 0.32247990369796753\n",
      "[step: 884] loss: 0.3223940134048462\n",
      "[step: 885] loss: 0.3223085105419159\n",
      "[step: 886] loss: 0.32222622632980347\n",
      "[step: 887] loss: 0.3221466541290283\n",
      "[step: 888] loss: 0.32206788659095764\n",
      "[step: 889] loss: 0.3219880163669586\n",
      "[step: 890] loss: 0.3219062387943268\n",
      "[step: 891] loss: 0.32182350754737854\n",
      "[step: 892] loss: 0.32174134254455566\n",
      "[step: 893] loss: 0.3216606378555298\n",
      "[step: 894] loss: 0.32158130407333374\n",
      "[step: 895] loss: 0.3215027153491974\n",
      "[step: 896] loss: 0.3214239180088043\n",
      "[step: 897] loss: 0.32134389877319336\n",
      "[step: 898] loss: 0.3212638199329376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 899] loss: 0.32118362188339233\n",
      "[step: 900] loss: 0.32110393047332764\n",
      "[step: 901] loss: 0.321025013923645\n",
      "[step: 902] loss: 0.32094693183898926\n",
      "[step: 903] loss: 0.32086867094039917\n",
      "[step: 904] loss: 0.32079067826271057\n",
      "[step: 905] loss: 0.3207123875617981\n",
      "[step: 906] loss: 0.32063400745391846\n",
      "[step: 907] loss: 0.32055577635765076\n",
      "[step: 908] loss: 0.320477694272995\n",
      "[step: 909] loss: 0.32039976119995117\n",
      "[step: 910] loss: 0.32032230496406555\n",
      "[step: 911] loss: 0.3202449679374695\n",
      "[step: 912] loss: 0.3201679289340973\n",
      "[step: 913] loss: 0.3200910985469818\n",
      "[step: 914] loss: 0.32001444697380066\n",
      "[step: 915] loss: 0.31993770599365234\n",
      "[step: 916] loss: 0.3198610842227936\n",
      "[step: 917] loss: 0.31978461146354675\n",
      "[step: 918] loss: 0.31970810890197754\n",
      "[step: 919] loss: 0.3196319043636322\n",
      "[step: 920] loss: 0.31955569982528687\n",
      "[step: 921] loss: 0.31947973370552063\n",
      "[step: 922] loss: 0.3194040060043335\n",
      "[step: 923] loss: 0.31932857632637024\n",
      "[step: 924] loss: 0.3192528486251831\n",
      "[step: 925] loss: 0.319177508354187\n",
      "[step: 926] loss: 0.3191022276878357\n",
      "[step: 927] loss: 0.31902748346328735\n",
      "[step: 928] loss: 0.3189525604248047\n",
      "[step: 929] loss: 0.3188777565956116\n",
      "[step: 930] loss: 0.31880319118499756\n",
      "[step: 931] loss: 0.31872883439064026\n",
      "[step: 932] loss: 0.3186544179916382\n",
      "[step: 933] loss: 0.3185803294181824\n",
      "[step: 934] loss: 0.31850627064704895\n",
      "[step: 935] loss: 0.31843245029449463\n",
      "[step: 936] loss: 0.31835874915122986\n",
      "[step: 937] loss: 0.31828558444976807\n",
      "[step: 938] loss: 0.3182123005390167\n",
      "[step: 939] loss: 0.31813955307006836\n",
      "[step: 940] loss: 0.3180673122406006\n",
      "[step: 941] loss: 0.31799575686454773\n",
      "[step: 942] loss: 0.3179255723953247\n",
      "[step: 943] loss: 0.31785711646080017\n",
      "[step: 944] loss: 0.31779181957244873\n",
      "[step: 945] loss: 0.31773146986961365\n",
      "[step: 946] loss: 0.31768035888671875\n",
      "[step: 947] loss: 0.3176450729370117\n",
      "[step: 948] loss: 0.31763848662376404\n",
      "[step: 949] loss: 0.3176848888397217\n",
      "[step: 950] loss: 0.3178274631500244\n",
      "[step: 951] loss: 0.3181561231613159\n",
      "[step: 952] loss: 0.3188289701938629\n",
      "[step: 953] loss: 0.32019078731536865\n",
      "[step: 954] loss: 0.32284072041511536\n",
      "[step: 955] loss: 0.32819491624832153\n",
      "[step: 956] loss: 0.33853158354759216\n",
      "[step: 957] loss: 0.3599816560745239\n",
      "[step: 958] loss: 0.4006154537200928\n",
      "[step: 959] loss: 0.4874474108219147\n",
      "[step: 960] loss: 0.6378196477890015\n",
      "[step: 961] loss: 0.9544152021408081\n",
      "[step: 962] loss: 1.330502986907959\n",
      "[step: 963] loss: 1.9379332065582275\n",
      "[step: 964] loss: 1.7516348361968994\n",
      "[step: 965] loss: 1.1987394094467163\n",
      "[step: 966] loss: 0.3998488783836365\n",
      "[step: 967] loss: 0.5818260312080383\n",
      "[step: 968] loss: 1.0914957523345947\n",
      "[step: 969] loss: 0.6529269814491272\n",
      "[step: 970] loss: 0.3666538596153259\n",
      "[step: 971] loss: 0.7108161449432373\n",
      "[step: 972] loss: 0.6176475286483765\n",
      "[step: 973] loss: 0.3520454168319702\n",
      "[step: 974] loss: 0.5277256369590759\n",
      "[step: 975] loss: 0.5371134281158447\n",
      "[step: 976] loss: 0.3496409058570862\n",
      "[step: 977] loss: 0.4406518340110779\n",
      "[step: 978] loss: 0.4832368791103363\n",
      "[step: 979] loss: 0.3539979159832001\n",
      "[step: 980] loss: 0.39105746150016785\n",
      "[step: 981] loss: 0.45146024227142334\n",
      "[step: 982] loss: 0.36705997586250305\n",
      "[step: 983] loss: 0.36038100719451904\n",
      "[step: 984] loss: 0.42092615365982056\n",
      "[step: 985] loss: 0.3743308186531067\n",
      "[step: 986] loss: 0.3422401249408722\n",
      "[step: 987] loss: 0.39046287536621094\n",
      "[step: 988] loss: 0.37276721000671387\n",
      "[step: 989] loss: 0.3324618637561798\n",
      "[step: 990] loss: 0.3644782304763794\n",
      "[step: 991] loss: 0.3672181963920593\n",
      "[step: 992] loss: 0.3302494287490845\n",
      "[step: 993] loss: 0.3460378646850586\n",
      "[step: 994] loss: 0.35998058319091797\n",
      "[step: 995] loss: 0.3317722976207733\n",
      "[step: 996] loss: 0.33306846022605896\n",
      "[step: 997] loss: 0.3505092263221741\n",
      "[step: 998] loss: 0.3342941999435425\n",
      "[step: 999] loss: 0.32441580295562744\n",
      "[step: 1000] loss: 0.33813878893852234\n",
      "[step: 1001] loss: 0.335775762796402\n",
      "[step: 1002] loss: 0.32217347621917725\n",
      "[step: 1003] loss: 0.3258005976676941\n",
      "[step: 1004] loss: 0.33327144384384155\n",
      "[step: 1005] loss: 0.3263924717903137\n",
      "[step: 1006] loss: 0.3198859989643097\n",
      "[step: 1007] loss: 0.325390487909317\n",
      "[step: 1008] loss: 0.32880550622940063\n",
      "[step: 1009] loss: 0.32268860936164856\n",
      "[step: 1010] loss: 0.3193690776824951\n",
      "[step: 1011] loss: 0.32341039180755615\n",
      "[step: 1012] loss: 0.3250238299369812\n",
      "[step: 1013] loss: 0.3209124803543091\n",
      "[step: 1014] loss: 0.3185522258281708\n",
      "[step: 1015] loss: 0.3210357427597046\n",
      "[step: 1016] loss: 0.3224720060825348\n",
      "[step: 1017] loss: 0.31993788480758667\n",
      "[step: 1018] loss: 0.3180684447288513\n",
      "[step: 1019] loss: 0.3194635510444641\n",
      "[step: 1020] loss: 0.3207290768623352\n",
      "[step: 1021] loss: 0.319414883852005\n",
      "[step: 1022] loss: 0.3178146481513977\n",
      "[step: 1023] loss: 0.31828662753105164\n",
      "[step: 1024] loss: 0.3193453550338745\n",
      "[step: 1025] loss: 0.31883978843688965\n",
      "[step: 1026] loss: 0.3175676465034485\n",
      "[step: 1027] loss: 0.31737053394317627\n",
      "[step: 1028] loss: 0.3180837035179138\n",
      "[step: 1029] loss: 0.31820017099380493\n",
      "[step: 1030] loss: 0.317421019077301\n",
      "[step: 1031] loss: 0.3168765902519226\n",
      "[step: 1032] loss: 0.3171278238296509\n",
      "[step: 1033] loss: 0.31746792793273926\n",
      "[step: 1034] loss: 0.31720998883247375\n",
      "[step: 1035] loss: 0.31665605306625366\n",
      "[step: 1036] loss: 0.31647831201553345\n",
      "[step: 1037] loss: 0.31668639183044434\n",
      "[step: 1038] loss: 0.31675127148628235\n",
      "[step: 1039] loss: 0.3164544105529785\n",
      "[step: 1040] loss: 0.3161214292049408\n",
      "[step: 1041] loss: 0.31607311964035034\n",
      "[step: 1042] loss: 0.3161854147911072\n",
      "[step: 1043] loss: 0.31614673137664795\n",
      "[step: 1044] loss: 0.3159150183200836\n",
      "[step: 1045] loss: 0.3157144784927368\n",
      "[step: 1046] loss: 0.315687358379364\n",
      "[step: 1047] loss: 0.31572431325912476\n",
      "[step: 1048] loss: 0.31565529108047485\n",
      "[step: 1049] loss: 0.31548240780830383\n",
      "[step: 1050] loss: 0.3153393864631653\n",
      "[step: 1051] loss: 0.31529855728149414\n",
      "[step: 1052] loss: 0.31529316306114197\n",
      "[step: 1053] loss: 0.31522834300994873\n",
      "[step: 1054] loss: 0.3151026666164398\n",
      "[step: 1055] loss: 0.31498774886131287\n",
      "[step: 1056] loss: 0.3149307370185852\n",
      "[step: 1057] loss: 0.314902663230896\n",
      "[step: 1058] loss: 0.31484800577163696\n",
      "[step: 1059] loss: 0.31475308537483215\n",
      "[step: 1060] loss: 0.314652681350708\n",
      "[step: 1061] loss: 0.3145807385444641\n",
      "[step: 1062] loss: 0.31453391909599304\n",
      "[step: 1063] loss: 0.3144833445549011\n",
      "[step: 1064] loss: 0.31441012024879456\n",
      "[step: 1065] loss: 0.3143237233161926\n",
      "[step: 1066] loss: 0.31424540281295776\n",
      "[step: 1067] loss: 0.31418460607528687\n",
      "[step: 1068] loss: 0.31413164734840393\n",
      "[step: 1069] loss: 0.31407174468040466\n",
      "[step: 1070] loss: 0.3139994740486145\n",
      "[step: 1071] loss: 0.3139230012893677\n",
      "[step: 1072] loss: 0.31385329365730286\n",
      "[step: 1073] loss: 0.31379228830337524\n",
      "[step: 1074] loss: 0.313734233379364\n",
      "[step: 1075] loss: 0.3136723041534424\n",
      "[step: 1076] loss: 0.3136044144630432\n",
      "[step: 1077] loss: 0.3135342001914978\n",
      "[step: 1078] loss: 0.31346726417541504\n",
      "[step: 1079] loss: 0.31340473890304565\n",
      "[step: 1080] loss: 0.31334489583969116\n",
      "[step: 1081] loss: 0.3132835030555725\n",
      "[step: 1082] loss: 0.31321972608566284\n",
      "[step: 1083] loss: 0.31315362453460693\n",
      "[step: 1084] loss: 0.3130881190299988\n",
      "[step: 1085] loss: 0.3130245804786682\n",
      "[step: 1086] loss: 0.3129630386829376\n",
      "[step: 1087] loss: 0.3129022419452667\n",
      "[step: 1088] loss: 0.3128409683704376\n",
      "[step: 1089] loss: 0.31277838349342346\n",
      "[step: 1090] loss: 0.31271466612815857\n",
      "[step: 1091] loss: 0.31265121698379517\n",
      "[step: 1092] loss: 0.3125891387462616\n",
      "[step: 1093] loss: 0.31252777576446533\n",
      "[step: 1094] loss: 0.31246691942214966\n",
      "[step: 1095] loss: 0.3124060332775116\n",
      "[step: 1096] loss: 0.3123449683189392\n",
      "[step: 1097] loss: 0.3122836649417877\n",
      "[step: 1098] loss: 0.31222182512283325\n",
      "[step: 1099] loss: 0.3121601939201355\n",
      "[step: 1100] loss: 0.3120991587638855\n",
      "[step: 1101] loss: 0.31203848123550415\n",
      "[step: 1102] loss: 0.31197798252105713\n",
      "[step: 1103] loss: 0.3119177520275116\n",
      "[step: 1104] loss: 0.3118577003479004\n",
      "[step: 1105] loss: 0.3117973208427429\n",
      "[step: 1106] loss: 0.311737060546875\n",
      "[step: 1107] loss: 0.31167668104171753\n",
      "[step: 1108] loss: 0.3116164803504944\n",
      "[step: 1109] loss: 0.3115563988685608\n",
      "[step: 1110] loss: 0.3114967346191406\n",
      "[step: 1111] loss: 0.3114369213581085\n",
      "[step: 1112] loss: 0.3113774359226227\n",
      "[step: 1113] loss: 0.31131815910339355\n",
      "[step: 1114] loss: 0.31125879287719727\n",
      "[step: 1115] loss: 0.31119978427886963\n",
      "[step: 1116] loss: 0.31114065647125244\n",
      "[step: 1117] loss: 0.3110816478729248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1118] loss: 0.3110223412513733\n",
      "[step: 1119] loss: 0.31096354126930237\n",
      "[step: 1120] loss: 0.31090474128723145\n",
      "[step: 1121] loss: 0.3108460307121277\n",
      "[step: 1122] loss: 0.31078726053237915\n",
      "[step: 1123] loss: 0.31072884798049927\n",
      "[step: 1124] loss: 0.3106704354286194\n",
      "[step: 1125] loss: 0.31061217188835144\n",
      "[step: 1126] loss: 0.31055402755737305\n",
      "[step: 1127] loss: 0.3104957044124603\n",
      "[step: 1128] loss: 0.31043803691864014\n",
      "[step: 1129] loss: 0.31037983298301697\n",
      "[step: 1130] loss: 0.31032198667526245\n",
      "[step: 1131] loss: 0.310264527797699\n",
      "[step: 1132] loss: 0.31020674109458923\n",
      "[step: 1133] loss: 0.3101491928100586\n",
      "[step: 1134] loss: 0.3100918233394623\n",
      "[step: 1135] loss: 0.3100343942642212\n",
      "[step: 1136] loss: 0.3099771738052368\n",
      "[step: 1137] loss: 0.30992019176483154\n",
      "[step: 1138] loss: 0.30986320972442627\n",
      "[step: 1139] loss: 0.309806227684021\n",
      "[step: 1140] loss: 0.3097493648529053\n",
      "[step: 1141] loss: 0.3096928596496582\n",
      "[step: 1142] loss: 0.30963611602783203\n",
      "[step: 1143] loss: 0.3095799684524536\n",
      "[step: 1144] loss: 0.309524267911911\n",
      "[step: 1145] loss: 0.3094692826271057\n",
      "[step: 1146] loss: 0.3094150424003601\n",
      "[step: 1147] loss: 0.30936262011528015\n",
      "[step: 1148] loss: 0.3093132972717285\n",
      "[step: 1149] loss: 0.3092697858810425\n",
      "[step: 1150] loss: 0.3092363774776459\n",
      "[step: 1151] loss: 0.309222549200058\n",
      "[step: 1152] loss: 0.30924564599990845\n",
      "[step: 1153] loss: 0.30933940410614014\n",
      "[step: 1154] loss: 0.3095738887786865\n",
      "[step: 1155] loss: 0.3100808560848236\n",
      "[step: 1156] loss: 0.3111479878425598\n",
      "[step: 1157] loss: 0.3133109211921692\n",
      "[step: 1158] loss: 0.31782346963882446\n",
      "[step: 1159] loss: 0.326893150806427\n",
      "[step: 1160] loss: 0.34628617763519287\n",
      "[step: 1161] loss: 0.38467806577682495\n",
      "[step: 1162] loss: 0.4693530797958374\n",
      "[step: 1163] loss: 0.6247392892837524\n",
      "[step: 1164] loss: 0.9666374921798706\n",
      "[step: 1165] loss: 1.4191796779632568\n",
      "[step: 1166] loss: 2.2192981243133545\n",
      "[step: 1167] loss: 2.1318933963775635\n",
      "[step: 1168] loss: 1.574822187423706\n",
      "[step: 1169] loss: 0.45288076996803284\n",
      "[step: 1170] loss: 0.6305317282676697\n",
      "[step: 1171] loss: 1.3029435873031616\n",
      "[step: 1172] loss: 0.6840395927429199\n",
      "[step: 1173] loss: 0.40894848108291626\n",
      "[step: 1174] loss: 0.8856079578399658\n",
      "[step: 1175] loss: 0.5870444774627686\n",
      "[step: 1176] loss: 0.38458460569381714\n",
      "[step: 1177] loss: 0.6880035400390625\n",
      "[step: 1178] loss: 0.46710705757141113\n",
      "[step: 1179] loss: 0.38585543632507324\n",
      "[step: 1180] loss: 0.5841733813285828\n",
      "[step: 1181] loss: 0.39954498410224915\n",
      "[step: 1182] loss: 0.3902941346168518\n",
      "[step: 1183] loss: 0.5231837034225464\n",
      "[step: 1184] loss: 0.3766782879829407\n",
      "[step: 1185] loss: 0.38964036107063293\n",
      "[step: 1186] loss: 0.4777757525444031\n",
      "[step: 1187] loss: 0.3626057505607605\n",
      "[step: 1188] loss: 0.3800771236419678\n",
      "[step: 1189] loss: 0.4376385509967804\n",
      "[step: 1190] loss: 0.3476178050041199\n",
      "[step: 1191] loss: 0.3664839267730713\n",
      "[step: 1192] loss: 0.40378835797309875\n",
      "[step: 1193] loss: 0.33555734157562256\n",
      "[step: 1194] loss: 0.35552552342414856\n",
      "[step: 1195] loss: 0.3807096481323242\n",
      "[step: 1196] loss: 0.32922741770744324\n",
      "[step: 1197] loss: 0.3469102382659912\n",
      "[step: 1198] loss: 0.3647075295448303\n",
      "[step: 1199] loss: 0.32560738921165466\n",
      "[step: 1200] loss: 0.3382229208946228\n",
      "[step: 1201] loss: 0.3526206910610199\n",
      "[step: 1202] loss: 0.32255613803863525\n",
      "[step: 1203] loss: 0.32777851819992065\n",
      "[step: 1204] loss: 0.34296849370002747\n",
      "[step: 1205] loss: 0.322306752204895\n",
      "[step: 1206] loss: 0.31878048181533813\n",
      "[step: 1207] loss: 0.33392131328582764\n",
      "[step: 1208] loss: 0.3246418833732605\n",
      "[step: 1209] loss: 0.3145042061805725\n",
      "[step: 1210] loss: 0.3245044946670532\n",
      "[step: 1211] loss: 0.32603204250335693\n",
      "[step: 1212] loss: 0.31536775827407837\n",
      "[step: 1213] loss: 0.3161931037902832\n",
      "[step: 1214] loss: 0.32286933064460754\n",
      "[step: 1215] loss: 0.318462997674942\n",
      "[step: 1216] loss: 0.31281954050064087\n",
      "[step: 1217] loss: 0.31683778762817383\n",
      "[step: 1218] loss: 0.3191335201263428\n",
      "[step: 1219] loss: 0.3141503930091858\n",
      "[step: 1220] loss: 0.3128323554992676\n",
      "[step: 1221] loss: 0.3164478540420532\n",
      "[step: 1222] loss: 0.3158954381942749\n",
      "[step: 1223] loss: 0.31247687339782715\n",
      "[step: 1224] loss: 0.3130265474319458\n",
      "[step: 1225] loss: 0.3151072859764099\n",
      "[step: 1226] loss: 0.3137279748916626\n",
      "[step: 1227] loss: 0.3116832375526428\n",
      "[step: 1228] loss: 0.3126028776168823\n",
      "[step: 1229] loss: 0.3136744499206543\n",
      "[step: 1230] loss: 0.3123525381088257\n",
      "[step: 1231] loss: 0.31122279167175293\n",
      "[step: 1232] loss: 0.31201958656311035\n",
      "[step: 1233] loss: 0.3125554025173187\n",
      "[step: 1234] loss: 0.3115867078304291\n",
      "[step: 1235] loss: 0.31088218092918396\n",
      "[step: 1236] loss: 0.31140050292015076\n",
      "[step: 1237] loss: 0.3116934597492218\n",
      "[step: 1238] loss: 0.3109971582889557\n",
      "[step: 1239] loss: 0.31049519777297974\n",
      "[step: 1240] loss: 0.3107999563217163\n",
      "[step: 1241] loss: 0.3109789490699768\n",
      "[step: 1242] loss: 0.3105217218399048\n",
      "[step: 1243] loss: 0.3101520240306854\n",
      "[step: 1244] loss: 0.31031668186187744\n",
      "[step: 1245] loss: 0.3104499876499176\n",
      "[step: 1246] loss: 0.31015366315841675\n",
      "[step: 1247] loss: 0.3098599314689636\n",
      "[step: 1248] loss: 0.30991530418395996\n",
      "[step: 1249] loss: 0.3100046217441559\n",
      "[step: 1250] loss: 0.3098207414150238\n",
      "[step: 1251] loss: 0.3095804750919342\n",
      "[step: 1252] loss: 0.30955877900123596\n",
      "[step: 1253] loss: 0.30961793661117554\n",
      "[step: 1254] loss: 0.30951744318008423\n",
      "[step: 1255] loss: 0.3093307614326477\n",
      "[step: 1256] loss: 0.3092595338821411\n",
      "[step: 1257] loss: 0.30928361415863037\n",
      "[step: 1258] loss: 0.3092362582683563\n",
      "[step: 1259] loss: 0.3090977370738983\n",
      "[step: 1260] loss: 0.30899757146835327\n",
      "[step: 1261] loss: 0.30898064374923706\n",
      "[step: 1262] loss: 0.3089548945426941\n",
      "[step: 1263] loss: 0.30886128544807434\n",
      "[step: 1264] loss: 0.30875706672668457\n",
      "[step: 1265] loss: 0.3087056279182434\n",
      "[step: 1266] loss: 0.30867984890937805\n",
      "[step: 1267] loss: 0.3086211085319519\n",
      "[step: 1268] loss: 0.30853167176246643\n",
      "[step: 1269] loss: 0.308459609746933\n",
      "[step: 1270] loss: 0.30841872096061707\n",
      "[step: 1271] loss: 0.3083757162094116\n",
      "[step: 1272] loss: 0.3083071708679199\n",
      "[step: 1273] loss: 0.3082311749458313\n",
      "[step: 1274] loss: 0.30817267298698425\n",
      "[step: 1275] loss: 0.30812811851501465\n",
      "[step: 1276] loss: 0.30807626247406006\n",
      "[step: 1277] loss: 0.3080097734928131\n",
      "[step: 1278] loss: 0.30794385075569153\n",
      "[step: 1279] loss: 0.3078900873661041\n",
      "[step: 1280] loss: 0.30784207582473755\n",
      "[step: 1281] loss: 0.3077876567840576\n",
      "[step: 1282] loss: 0.30772578716278076\n",
      "[step: 1283] loss: 0.30766499042510986\n",
      "[step: 1284] loss: 0.30761125683784485\n",
      "[step: 1285] loss: 0.3075607419013977\n",
      "[step: 1286] loss: 0.3075070083141327\n",
      "[step: 1287] loss: 0.30744847655296326\n",
      "[step: 1288] loss: 0.3073906898498535\n",
      "[step: 1289] loss: 0.3073367476463318\n",
      "[step: 1290] loss: 0.307285338640213\n",
      "[step: 1291] loss: 0.30723243951797485\n",
      "[step: 1292] loss: 0.3071768283843994\n",
      "[step: 1293] loss: 0.30712050199508667\n",
      "[step: 1294] loss: 0.3070666193962097\n",
      "[step: 1295] loss: 0.30701446533203125\n",
      "[step: 1296] loss: 0.306962251663208\n",
      "[step: 1297] loss: 0.30690857768058777\n",
      "[step: 1298] loss: 0.3068539500236511\n",
      "[step: 1299] loss: 0.3068000078201294\n",
      "[step: 1300] loss: 0.3067474067211151\n",
      "[step: 1301] loss: 0.3066955804824829\n",
      "[step: 1302] loss: 0.30664318799972534\n",
      "[step: 1303] loss: 0.30659013986587524\n",
      "[step: 1304] loss: 0.3065369427204132\n",
      "[step: 1305] loss: 0.3064841032028198\n",
      "[step: 1306] loss: 0.3064320981502533\n",
      "[step: 1307] loss: 0.3063805103302002\n",
      "[step: 1308] loss: 0.3063286542892456\n",
      "[step: 1309] loss: 0.30627626180648804\n",
      "[step: 1310] loss: 0.30622416734695435\n",
      "[step: 1311] loss: 0.30617207288742065\n",
      "[step: 1312] loss: 0.30612054467201233\n",
      "[step: 1313] loss: 0.3060692548751831\n",
      "[step: 1314] loss: 0.30601778626441956\n",
      "[step: 1315] loss: 0.30596640706062317\n",
      "[step: 1316] loss: 0.30591511726379395\n",
      "[step: 1317] loss: 0.30586349964141846\n",
      "[step: 1318] loss: 0.3058125376701355\n",
      "[step: 1319] loss: 0.3057616353034973\n",
      "[step: 1320] loss: 0.30571073293685913\n",
      "[step: 1321] loss: 0.30565980076789856\n",
      "[step: 1322] loss: 0.3056092858314514\n",
      "[step: 1323] loss: 0.30555832386016846\n",
      "[step: 1324] loss: 0.3055077791213989\n",
      "[step: 1325] loss: 0.3054574131965637\n",
      "[step: 1326] loss: 0.3054070472717285\n",
      "[step: 1327] loss: 0.3053567111492157\n",
      "[step: 1328] loss: 0.3053065240383148\n",
      "[step: 1329] loss: 0.30525636672973633\n",
      "[step: 1330] loss: 0.3052063584327698\n",
      "[step: 1331] loss: 0.30515626072883606\n",
      "[step: 1332] loss: 0.30510640144348145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1333] loss: 0.30505669116973877\n",
      "[step: 1334] loss: 0.3050069808959961\n",
      "[step: 1335] loss: 0.3049575686454773\n",
      "[step: 1336] loss: 0.3049079179763794\n",
      "[step: 1337] loss: 0.3048584461212158\n",
      "[step: 1338] loss: 0.304809033870697\n",
      "[step: 1339] loss: 0.3047597408294678\n",
      "[step: 1340] loss: 0.3047105371952057\n",
      "[step: 1341] loss: 0.30466169118881226\n",
      "[step: 1342] loss: 0.3046126663684845\n",
      "[step: 1343] loss: 0.3045635223388672\n",
      "[step: 1344] loss: 0.30451473593711853\n",
      "[step: 1345] loss: 0.304465651512146\n",
      "[step: 1346] loss: 0.30441707372665405\n",
      "[step: 1347] loss: 0.30436840653419495\n",
      "[step: 1348] loss: 0.3043198585510254\n",
      "[step: 1349] loss: 0.3042713403701782\n",
      "[step: 1350] loss: 0.304222971200943\n",
      "[step: 1351] loss: 0.30417463183403015\n",
      "[step: 1352] loss: 0.304126501083374\n",
      "[step: 1353] loss: 0.3040781617164612\n",
      "[step: 1354] loss: 0.30403006076812744\n",
      "[step: 1355] loss: 0.303982138633728\n",
      "[step: 1356] loss: 0.3039341866970062\n",
      "[step: 1357] loss: 0.3038863241672516\n",
      "[step: 1358] loss: 0.3038386106491089\n",
      "[step: 1359] loss: 0.3037909269332886\n",
      "[step: 1360] loss: 0.30374324321746826\n",
      "[step: 1361] loss: 0.30369576811790466\n",
      "[step: 1362] loss: 0.3036482036113739\n",
      "[step: 1363] loss: 0.3036006987094879\n",
      "[step: 1364] loss: 0.3035533130168915\n",
      "[step: 1365] loss: 0.3035062551498413\n",
      "[step: 1366] loss: 0.3034588396549225\n",
      "[step: 1367] loss: 0.3034118711948395\n",
      "[step: 1368] loss: 0.30336472392082214\n",
      "[step: 1369] loss: 0.3033179044723511\n",
      "[step: 1370] loss: 0.30327099561691284\n",
      "[step: 1371] loss: 0.3032241463661194\n",
      "[step: 1372] loss: 0.3031773567199707\n",
      "[step: 1373] loss: 0.3031308948993683\n",
      "[step: 1374] loss: 0.30308425426483154\n",
      "[step: 1375] loss: 0.3030376434326172\n",
      "[step: 1376] loss: 0.30299124121665955\n",
      "[step: 1377] loss: 0.3029446005821228\n",
      "[step: 1378] loss: 0.3028985857963562\n",
      "[step: 1379] loss: 0.30285215377807617\n",
      "[step: 1380] loss: 0.30280601978302\n",
      "[step: 1381] loss: 0.30275994539260864\n",
      "[step: 1382] loss: 0.3027139902114868\n",
      "[step: 1383] loss: 0.3026679456233978\n",
      "[step: 1384] loss: 0.30262210965156555\n",
      "[step: 1385] loss: 0.30257630348205566\n",
      "[step: 1386] loss: 0.3025306463241577\n",
      "[step: 1387] loss: 0.3024851679801941\n",
      "[step: 1388] loss: 0.30243974924087524\n",
      "[step: 1389] loss: 0.3023945689201355\n",
      "[step: 1390] loss: 0.3023497462272644\n",
      "[step: 1391] loss: 0.30230528116226196\n",
      "[step: 1392] loss: 0.30226224660873413\n",
      "[step: 1393] loss: 0.30222076177597046\n",
      "[step: 1394] loss: 0.3021829426288605\n",
      "[step: 1395] loss: 0.3021519184112549\n",
      "[step: 1396] loss: 0.3021341860294342\n",
      "[step: 1397] loss: 0.3021427392959595\n",
      "[step: 1398] loss: 0.3022039532661438\n",
      "[step: 1399] loss: 0.3023715615272522\n",
      "[step: 1400] loss: 0.30276039242744446\n",
      "[step: 1401] loss: 0.303599089384079\n",
      "[step: 1402] loss: 0.305406391620636\n",
      "[step: 1403] loss: 0.3091912567615509\n",
      "[step: 1404] loss: 0.3174206018447876\n",
      "[step: 1405] loss: 0.33454176783561707\n",
      "[step: 1406] loss: 0.3728511929512024\n",
      "[step: 1407] loss: 0.45014622807502747\n",
      "[step: 1408] loss: 0.6276312470436096\n",
      "[step: 1409] loss: 0.9389450550079346\n",
      "[step: 1410] loss: 1.6172820329666138\n",
      "[step: 1411] loss: 2.2161455154418945\n",
      "[step: 1412] loss: 2.9352874755859375\n",
      "[step: 1413] loss: 1.6645138263702393\n",
      "[step: 1414] loss: 0.4661513566970825\n",
      "[step: 1415] loss: 0.6515679359436035\n",
      "[step: 1416] loss: 1.2501542568206787\n",
      "[step: 1417] loss: 0.7926968336105347\n",
      "[step: 1418] loss: 0.40635958313941956\n",
      "[step: 1419] loss: 0.9654445648193359\n",
      "[step: 1420] loss: 0.7404201030731201\n",
      "[step: 1421] loss: 0.38767725229263306\n",
      "[step: 1422] loss: 0.8479617834091187\n",
      "[step: 1423] loss: 0.5928913950920105\n",
      "[step: 1424] loss: 0.4045422673225403\n",
      "[step: 1425] loss: 0.7594655156135559\n",
      "[step: 1426] loss: 0.47370442748069763\n",
      "[step: 1427] loss: 0.4262733459472656\n",
      "[step: 1428] loss: 0.6650375723838806\n",
      "[step: 1429] loss: 0.4088519811630249\n",
      "[step: 1430] loss: 0.43685421347618103\n",
      "[step: 1431] loss: 0.5655288696289062\n",
      "[step: 1432] loss: 0.36449888348579407\n",
      "[step: 1433] loss: 0.43555426597595215\n",
      "[step: 1434] loss: 0.4732452630996704\n",
      "[step: 1435] loss: 0.33765560388565063\n",
      "[step: 1436] loss: 0.4217349588871002\n",
      "[step: 1437] loss: 0.3996182680130005\n",
      "[step: 1438] loss: 0.33156153559684753\n",
      "[step: 1439] loss: 0.40276747941970825\n",
      "[step: 1440] loss: 0.3535776734352112\n",
      "[step: 1441] loss: 0.3360717296600342\n",
      "[step: 1442] loss: 0.3808404803276062\n",
      "[step: 1443] loss: 0.32979077100753784\n",
      "[step: 1444] loss: 0.339915931224823\n",
      "[step: 1445] loss: 0.35973483324050903\n",
      "[step: 1446] loss: 0.31813910603523254\n",
      "[step: 1447] loss: 0.3366556167602539\n",
      "[step: 1448] loss: 0.3440887928009033\n",
      "[step: 1449] loss: 0.3130752146244049\n",
      "[step: 1450] loss: 0.3300411105155945\n",
      "[step: 1451] loss: 0.33477771282196045\n",
      "[step: 1452] loss: 0.3112700581550598\n",
      "[step: 1453] loss: 0.3235117793083191\n",
      "[step: 1454] loss: 0.3296964764595032\n",
      "[step: 1455] loss: 0.3111834228038788\n",
      "[step: 1456] loss: 0.31709519028663635\n",
      "[step: 1457] loss: 0.3251980245113373\n",
      "[step: 1458] loss: 0.3117211163043976\n",
      "[step: 1459] loss: 0.31179946660995483\n",
      "[step: 1460] loss: 0.32028141617774963\n",
      "[step: 1461] loss: 0.312347948551178\n",
      "[step: 1462] loss: 0.30873724818229675\n",
      "[step: 1463] loss: 0.315805047750473\n",
      "[step: 1464] loss: 0.31284525990486145\n",
      "[step: 1465] loss: 0.3076789379119873\n",
      "[step: 1466] loss: 0.3119603097438812\n",
      "[step: 1467] loss: 0.31255263090133667\n",
      "[step: 1468] loss: 0.3077431321144104\n",
      "[step: 1469] loss: 0.3088887929916382\n",
      "[step: 1470] loss: 0.3112151622772217\n",
      "[step: 1471] loss: 0.3081871271133423\n",
      "[step: 1472] loss: 0.30699118971824646\n",
      "[step: 1473] loss: 0.30932682752609253\n",
      "[step: 1474] loss: 0.3085334897041321\n",
      "[step: 1475] loss: 0.3063742220401764\n",
      "[step: 1476] loss: 0.3074539601802826\n",
      "[step: 1477] loss: 0.3082449436187744\n",
      "[step: 1478] loss: 0.30653586983680725\n",
      "[step: 1479] loss: 0.30613040924072266\n",
      "[step: 1480] loss: 0.30721789598464966\n",
      "[step: 1481] loss: 0.30666854977607727\n",
      "[step: 1482] loss: 0.30558598041534424\n",
      "[step: 1483] loss: 0.3060460686683655\n",
      "[step: 1484] loss: 0.3063821792602539\n",
      "[step: 1485] loss: 0.30555135011672974\n",
      "[step: 1486] loss: 0.3052690923213959\n",
      "[step: 1487] loss: 0.3057635426521301\n",
      "[step: 1488] loss: 0.30555880069732666\n",
      "[step: 1489] loss: 0.3049737811088562\n",
      "[step: 1490] loss: 0.30509719252586365\n",
      "[step: 1491] loss: 0.30530354380607605\n",
      "[step: 1492] loss: 0.30491548776626587\n",
      "[step: 1493] loss: 0.3046552538871765\n",
      "[step: 1494] loss: 0.3048439025878906\n",
      "[step: 1495] loss: 0.3048088848590851\n",
      "[step: 1496] loss: 0.3044849932193756\n",
      "[step: 1497] loss: 0.30442696809768677\n",
      "[step: 1498] loss: 0.3045419454574585\n",
      "[step: 1499] loss: 0.30440622568130493\n",
      "[step: 1500] loss: 0.30419379472732544\n",
      "[step: 1501] loss: 0.30420583486557007\n",
      "[step: 1502] loss: 0.3042341470718384\n",
      "[step: 1503] loss: 0.3040788769721985\n",
      "[step: 1504] loss: 0.30394893884658813\n",
      "[step: 1505] loss: 0.3039650321006775\n",
      "[step: 1506] loss: 0.30393773317337036\n",
      "[step: 1507] loss: 0.3038027286529541\n",
      "[step: 1508] loss: 0.3037213087081909\n",
      "[step: 1509] loss: 0.3037222623825073\n",
      "[step: 1510] loss: 0.30367106199264526\n",
      "[step: 1511] loss: 0.30356118083000183\n",
      "[step: 1512] loss: 0.30350160598754883\n",
      "[step: 1513] loss: 0.3034849166870117\n",
      "[step: 1514] loss: 0.30342453718185425\n",
      "[step: 1515] loss: 0.30333423614501953\n",
      "[step: 1516] loss: 0.3032820522785187\n",
      "[step: 1517] loss: 0.3032533526420593\n",
      "[step: 1518] loss: 0.3031933605670929\n",
      "[step: 1519] loss: 0.303117036819458\n",
      "[step: 1520] loss: 0.30306708812713623\n",
      "[step: 1521] loss: 0.30303114652633667\n",
      "[step: 1522] loss: 0.3029748499393463\n",
      "[step: 1523] loss: 0.3029076159000397\n",
      "[step: 1524] loss: 0.302856981754303\n",
      "[step: 1525] loss: 0.30281662940979004\n",
      "[step: 1526] loss: 0.30276361107826233\n",
      "[step: 1527] loss: 0.3027022182941437\n",
      "[step: 1528] loss: 0.3026510775089264\n",
      "[step: 1529] loss: 0.3026078939437866\n",
      "[step: 1530] loss: 0.3025577664375305\n",
      "[step: 1531] loss: 0.3025011420249939\n",
      "[step: 1532] loss: 0.30244946479797363\n",
      "[step: 1533] loss: 0.30240458250045776\n",
      "[step: 1534] loss: 0.302356481552124\n",
      "[step: 1535] loss: 0.3023030757904053\n",
      "[step: 1536] loss: 0.30225175619125366\n",
      "[step: 1537] loss: 0.302204966545105\n",
      "[step: 1538] loss: 0.30215802788734436\n",
      "[step: 1539] loss: 0.3021075129508972\n",
      "[step: 1540] loss: 0.30205658078193665\n",
      "[step: 1541] loss: 0.30200886726379395\n",
      "[step: 1542] loss: 0.3019621968269348\n",
      "[step: 1543] loss: 0.3019137978553772\n",
      "[step: 1544] loss: 0.3018641471862793\n",
      "[step: 1545] loss: 0.30181562900543213\n",
      "[step: 1546] loss: 0.3017689883708954\n",
      "[step: 1547] loss: 0.30172199010849\n",
      "[step: 1548] loss: 0.3016732931137085\n",
      "[step: 1549] loss: 0.30162492394447327\n",
      "[step: 1550] loss: 0.30157798528671265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1551] loss: 0.3015313446521759\n",
      "[step: 1552] loss: 0.30148422718048096\n",
      "[step: 1553] loss: 0.3014364540576935\n",
      "[step: 1554] loss: 0.30138924717903137\n",
      "[step: 1555] loss: 0.3013429045677185\n",
      "[step: 1556] loss: 0.3012964725494385\n",
      "[step: 1557] loss: 0.3012496531009674\n",
      "[step: 1558] loss: 0.3012024760246277\n",
      "[step: 1559] loss: 0.301156222820282\n",
      "[step: 1560] loss: 0.3011102080345154\n",
      "[step: 1561] loss: 0.3010639250278473\n",
      "[step: 1562] loss: 0.301017701625824\n",
      "[step: 1563] loss: 0.30097126960754395\n",
      "[step: 1564] loss: 0.30092546343803406\n",
      "[step: 1565] loss: 0.30087971687316895\n",
      "[step: 1566] loss: 0.3008340001106262\n",
      "[step: 1567] loss: 0.3007880449295044\n",
      "[step: 1568] loss: 0.30074241757392883\n",
      "[step: 1569] loss: 0.30069687962532043\n",
      "[step: 1570] loss: 0.3006516098976135\n",
      "[step: 1571] loss: 0.30060622096061707\n",
      "[step: 1572] loss: 0.30056095123291016\n",
      "[step: 1573] loss: 0.30051565170288086\n",
      "[step: 1574] loss: 0.30047059059143066\n",
      "[step: 1575] loss: 0.30042564868927\n",
      "[step: 1576] loss: 0.3003807067871094\n",
      "[step: 1577] loss: 0.3003360331058502\n",
      "[step: 1578] loss: 0.30029118061065674\n",
      "[step: 1579] loss: 0.30024653673171997\n",
      "[step: 1580] loss: 0.30020207166671753\n",
      "[step: 1581] loss: 0.30015742778778076\n",
      "[step: 1582] loss: 0.30011308193206787\n",
      "[step: 1583] loss: 0.30006855726242065\n",
      "[step: 1584] loss: 0.3000243306159973\n",
      "[step: 1585] loss: 0.2999803125858307\n",
      "[step: 1586] loss: 0.2999362647533417\n",
      "[step: 1587] loss: 0.29989224672317505\n",
      "[step: 1588] loss: 0.2998482286930084\n",
      "[step: 1589] loss: 0.2998044490814209\n",
      "[step: 1590] loss: 0.2997605502605438\n",
      "[step: 1591] loss: 0.2997167706489563\n",
      "[step: 1592] loss: 0.29967325925827026\n",
      "[step: 1593] loss: 0.29962968826293945\n",
      "[step: 1594] loss: 0.29958614706993103\n",
      "[step: 1595] loss: 0.29954272508621216\n",
      "[step: 1596] loss: 0.2994993031024933\n",
      "[step: 1597] loss: 0.2994561195373535\n",
      "[step: 1598] loss: 0.2994130253791809\n",
      "[step: 1599] loss: 0.29937005043029785\n",
      "[step: 1600] loss: 0.29932695627212524\n",
      "[step: 1601] loss: 0.29928404092788696\n",
      "[step: 1602] loss: 0.2992410659790039\n",
      "[step: 1603] loss: 0.2991983890533447\n",
      "[step: 1604] loss: 0.2991555333137512\n",
      "[step: 1605] loss: 0.2991129755973816\n",
      "[step: 1606] loss: 0.2990705966949463\n",
      "[step: 1607] loss: 0.2990279793739319\n",
      "[step: 1608] loss: 0.29898548126220703\n",
      "[step: 1609] loss: 0.2989431917667389\n",
      "[step: 1610] loss: 0.29890093207359314\n",
      "[step: 1611] loss: 0.29885852336883545\n",
      "[step: 1612] loss: 0.29881662130355835\n",
      "[step: 1613] loss: 0.2987746000289917\n",
      "[step: 1614] loss: 0.2987323999404907\n",
      "[step: 1615] loss: 0.29869043827056885\n",
      "[step: 1616] loss: 0.2986486554145813\n",
      "[step: 1617] loss: 0.29860687255859375\n",
      "[step: 1618] loss: 0.2985653579235077\n",
      "[step: 1619] loss: 0.2985236346721649\n",
      "[step: 1620] loss: 0.29848217964172363\n",
      "[step: 1621] loss: 0.29844045639038086\n",
      "[step: 1622] loss: 0.2983991801738739\n",
      "[step: 1623] loss: 0.29835766553878784\n",
      "[step: 1624] loss: 0.2983165979385376\n",
      "[step: 1625] loss: 0.2982751131057739\n",
      "[step: 1626] loss: 0.2982339859008789\n",
      "[step: 1627] loss: 0.2981930375099182\n",
      "[step: 1628] loss: 0.29815202951431274\n",
      "[step: 1629] loss: 0.2981109023094177\n",
      "[step: 1630] loss: 0.2980700731277466\n",
      "[step: 1631] loss: 0.2980292737483978\n",
      "[step: 1632] loss: 0.2979884147644043\n",
      "[step: 1633] loss: 0.2979477643966675\n",
      "[step: 1634] loss: 0.29790711402893066\n",
      "[step: 1635] loss: 0.2978666424751282\n",
      "[step: 1636] loss: 0.2978260815143585\n",
      "[step: 1637] loss: 0.2977856397628784\n",
      "[step: 1638] loss: 0.29774534702301025\n",
      "[step: 1639] loss: 0.2977049946784973\n",
      "[step: 1640] loss: 0.2976648211479187\n",
      "[step: 1641] loss: 0.29762452840805054\n",
      "[step: 1642] loss: 0.29758453369140625\n",
      "[step: 1643] loss: 0.29754436016082764\n",
      "[step: 1644] loss: 0.29750436544418335\n",
      "[step: 1645] loss: 0.2974645495414734\n",
      "[step: 1646] loss: 0.29742470383644104\n",
      "[step: 1647] loss: 0.2973848581314087\n",
      "[step: 1648] loss: 0.2973451018333435\n",
      "[step: 1649] loss: 0.29730525612831116\n",
      "[step: 1650] loss: 0.297265887260437\n",
      "[step: 1651] loss: 0.2972261905670166\n",
      "[step: 1652] loss: 0.2971867322921753\n",
      "[step: 1653] loss: 0.29714739322662354\n",
      "[step: 1654] loss: 0.2971080243587494\n",
      "[step: 1655] loss: 0.29706865549087524\n",
      "[step: 1656] loss: 0.2970294952392578\n",
      "[step: 1657] loss: 0.2969902455806732\n",
      "[step: 1658] loss: 0.29695120453834534\n",
      "[step: 1659] loss: 0.29691219329833984\n",
      "[step: 1660] loss: 0.2968730330467224\n",
      "[step: 1661] loss: 0.2968343198299408\n",
      "[step: 1662] loss: 0.29679542779922485\n",
      "[step: 1663] loss: 0.2967565655708313\n",
      "[step: 1664] loss: 0.2967178523540497\n",
      "[step: 1665] loss: 0.2966790199279785\n",
      "[step: 1666] loss: 0.2966405749320984\n",
      "[step: 1667] loss: 0.29660192131996155\n",
      "[step: 1668] loss: 0.2965635061264038\n",
      "[step: 1669] loss: 0.2965250313282013\n",
      "[step: 1670] loss: 0.296486496925354\n",
      "[step: 1671] loss: 0.2964482307434082\n",
      "[step: 1672] loss: 0.29641005396842957\n",
      "[step: 1673] loss: 0.2963716387748718\n",
      "[step: 1674] loss: 0.29633355140686035\n",
      "[step: 1675] loss: 0.29629552364349365\n",
      "[step: 1676] loss: 0.29625746607780457\n",
      "[step: 1677] loss: 0.29621952772140503\n",
      "[step: 1678] loss: 0.2961817681789398\n",
      "[step: 1679] loss: 0.2961440980434418\n",
      "[step: 1680] loss: 0.29610657691955566\n",
      "[step: 1681] loss: 0.29606956243515015\n",
      "[step: 1682] loss: 0.29603296518325806\n",
      "[step: 1683] loss: 0.2959973216056824\n",
      "[step: 1684] loss: 0.2959631383419037\n",
      "[step: 1685] loss: 0.29593226313591003\n",
      "[step: 1686] loss: 0.2959067225456238\n",
      "[step: 1687] loss: 0.29589220881462097\n",
      "[step: 1688] loss: 0.295897901058197\n",
      "[step: 1689] loss: 0.29594331979751587\n",
      "[step: 1690] loss: 0.29606592655181885\n",
      "[step: 1691] loss: 0.2963433861732483\n",
      "[step: 1692] loss: 0.296925812959671\n",
      "[step: 1693] loss: 0.2981424629688263\n",
      "[step: 1694] loss: 0.30061301589012146\n",
      "[step: 1695] loss: 0.30579352378845215\n",
      "[step: 1696] loss: 0.31626346707344055\n",
      "[step: 1697] loss: 0.3387957811355591\n",
      "[step: 1698] loss: 0.3835275173187256\n",
      "[step: 1699] loss: 0.48271915316581726\n",
      "[step: 1700] loss: 0.6637855768203735\n",
      "[step: 1701] loss: 1.0629656314849854\n",
      "[step: 1702] loss: 1.5739904642105103\n",
      "[step: 1703] loss: 2.461202621459961\n",
      "[step: 1704] loss: 2.2622032165527344\n",
      "[step: 1705] loss: 1.5295281410217285\n",
      "[step: 1706] loss: 0.40015923976898193\n",
      "[step: 1707] loss: 0.7496854066848755\n",
      "[step: 1708] loss: 1.4144846200942993\n",
      "[step: 1709] loss: 0.6257383823394775\n",
      "[step: 1710] loss: 0.4730087220668793\n",
      "[step: 1711] loss: 1.0049554109573364\n",
      "[step: 1712] loss: 0.530261754989624\n",
      "[step: 1713] loss: 0.44337227940559387\n",
      "[step: 1714] loss: 0.7841403484344482\n",
      "[step: 1715] loss: 0.4139488637447357\n",
      "[step: 1716] loss: 0.4593324661254883\n",
      "[step: 1717] loss: 0.657926082611084\n",
      "[step: 1718] loss: 0.3605591952800751\n",
      "[step: 1719] loss: 0.475146621465683\n",
      "[step: 1720] loss: 0.5809342265129089\n",
      "[step: 1721] loss: 0.3451111912727356\n",
      "[step: 1722] loss: 0.4741278886795044\n",
      "[step: 1723] loss: 0.5129703879356384\n",
      "[step: 1724] loss: 0.3344328999519348\n",
      "[step: 1725] loss: 0.4614710807800293\n",
      "[step: 1726] loss: 0.4486311674118042\n",
      "[step: 1727] loss: 0.32480594515800476\n",
      "[step: 1728] loss: 0.44120293855667114\n",
      "[step: 1729] loss: 0.3980904221534729\n",
      "[step: 1730] loss: 0.32402169704437256\n",
      "[step: 1731] loss: 0.4191741347312927\n",
      "[step: 1732] loss: 0.3618389964103699\n",
      "[step: 1733] loss: 0.32659727334976196\n",
      "[step: 1734] loss: 0.39623332023620605\n",
      "[step: 1735] loss: 0.3383580148220062\n",
      "[step: 1736] loss: 0.3238598108291626\n",
      "[step: 1737] loss: 0.3728770613670349\n",
      "[step: 1738] loss: 0.3256341516971588\n",
      "[step: 1739] loss: 0.3164573013782501\n",
      "[step: 1740] loss: 0.35403162240982056\n",
      "[step: 1741] loss: 0.32138291001319885\n",
      "[step: 1742] loss: 0.30923163890838623\n",
      "[step: 1743] loss: 0.34028077125549316\n",
      "[step: 1744] loss: 0.322239488363266\n",
      "[step: 1745] loss: 0.30457833409309387\n",
      "[step: 1746] loss: 0.32789307832717896\n",
      "[step: 1747] loss: 0.3230164647102356\n",
      "[step: 1748] loss: 0.3028089702129364\n",
      "[step: 1749] loss: 0.31634873151779175\n",
      "[step: 1750] loss: 0.32099515199661255\n",
      "[step: 1751] loss: 0.3036428689956665\n",
      "[step: 1752] loss: 0.3080960512161255\n",
      "[step: 1753] loss: 0.31711655855178833\n",
      "[step: 1754] loss: 0.30567383766174316\n",
      "[step: 1755] loss: 0.3034071922302246\n",
      "[step: 1756] loss: 0.31223899126052856\n",
      "[step: 1757] loss: 0.3071713447570801\n",
      "[step: 1758] loss: 0.30130159854888916\n",
      "[step: 1759] loss: 0.3070172965526581\n",
      "[step: 1760] loss: 0.30716848373413086\n",
      "[step: 1761] loss: 0.30113857984542847\n",
      "[step: 1762] loss: 0.30281081795692444\n",
      "[step: 1763] loss: 0.3058128356933594\n",
      "[step: 1764] loss: 0.302114337682724\n",
      "[step: 1765] loss: 0.3005059063434601\n",
      "[step: 1766] loss: 0.3035082519054413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1767] loss: 0.3028462529182434\n",
      "[step: 1768] loss: 0.29999443888664246\n",
      "[step: 1769] loss: 0.3010973334312439\n",
      "[step: 1770] loss: 0.3023602068424225\n",
      "[step: 1771] loss: 0.300361305475235\n",
      "[step: 1772] loss: 0.29960304498672485\n",
      "[step: 1773] loss: 0.30102163553237915\n",
      "[step: 1774] loss: 0.30063316226005554\n",
      "[step: 1775] loss: 0.299238383769989\n",
      "[step: 1776] loss: 0.299707293510437\n",
      "[step: 1777] loss: 0.3003304898738861\n",
      "[step: 1778] loss: 0.29941874742507935\n",
      "[step: 1779] loss: 0.2989386320114136\n",
      "[step: 1780] loss: 0.29956960678100586\n",
      "[step: 1781] loss: 0.2994835078716278\n",
      "[step: 1782] loss: 0.29874226450920105\n",
      "[step: 1783] loss: 0.29881149530410767\n",
      "[step: 1784] loss: 0.2991734743118286\n",
      "[step: 1785] loss: 0.2988046407699585\n",
      "[step: 1786] loss: 0.2984241545200348\n",
      "[step: 1787] loss: 0.29864636063575745\n",
      "[step: 1788] loss: 0.29872798919677734\n",
      "[step: 1789] loss: 0.2983659505844116\n",
      "[step: 1790] loss: 0.29822713136672974\n",
      "[step: 1791] loss: 0.29839956760406494\n",
      "[step: 1792] loss: 0.2983270585536957\n",
      "[step: 1793] loss: 0.2980569899082184\n",
      "[step: 1794] loss: 0.2980305254459381\n",
      "[step: 1795] loss: 0.29812073707580566\n",
      "[step: 1796] loss: 0.2979966700077057\n",
      "[step: 1797] loss: 0.29782095551490784\n",
      "[step: 1798] loss: 0.2978295385837555\n",
      "[step: 1799] loss: 0.2978544235229492\n",
      "[step: 1800] loss: 0.2977302670478821\n",
      "[step: 1801] loss: 0.29761403799057007\n",
      "[step: 1802] loss: 0.29761937260627747\n",
      "[step: 1803] loss: 0.2976059913635254\n",
      "[step: 1804] loss: 0.29749777913093567\n",
      "[step: 1805] loss: 0.2974151372909546\n",
      "[step: 1806] loss: 0.2974087595939636\n",
      "[step: 1807] loss: 0.2973787188529968\n",
      "[step: 1808] loss: 0.29728978872299194\n",
      "[step: 1809] loss: 0.2972240746021271\n",
      "[step: 1810] loss: 0.29720625281333923\n",
      "[step: 1811] loss: 0.29716989398002625\n",
      "[step: 1812] loss: 0.2970956563949585\n",
      "[step: 1813] loss: 0.29703712463378906\n",
      "[step: 1814] loss: 0.2970101237297058\n",
      "[step: 1815] loss: 0.2969719171524048\n",
      "[step: 1816] loss: 0.2969089448451996\n",
      "[step: 1817] loss: 0.2968541979789734\n",
      "[step: 1818] loss: 0.2968207597732544\n",
      "[step: 1819] loss: 0.2967827618122101\n",
      "[step: 1820] loss: 0.29672771692276\n",
      "[step: 1821] loss: 0.29667553305625916\n",
      "[step: 1822] loss: 0.2966375946998596\n",
      "[step: 1823] loss: 0.2965995669364929\n",
      "[step: 1824] loss: 0.29655009508132935\n",
      "[step: 1825] loss: 0.29649966955184937\n",
      "[step: 1826] loss: 0.2964586019515991\n",
      "[step: 1827] loss: 0.29642021656036377\n",
      "[step: 1828] loss: 0.29637497663497925\n",
      "[step: 1829] loss: 0.2963266968727112\n",
      "[step: 1830] loss: 0.29628366231918335\n",
      "[step: 1831] loss: 0.2962443232536316\n",
      "[step: 1832] loss: 0.29620182514190674\n",
      "[step: 1833] loss: 0.29615601897239685\n",
      "[step: 1834] loss: 0.2961118817329407\n",
      "[step: 1835] loss: 0.29607129096984863\n",
      "[step: 1836] loss: 0.29603034257888794\n",
      "[step: 1837] loss: 0.2959865927696228\n",
      "[step: 1838] loss: 0.29594260454177856\n",
      "[step: 1839] loss: 0.29590117931365967\n",
      "[step: 1840] loss: 0.29586029052734375\n",
      "[step: 1841] loss: 0.29581838846206665\n",
      "[step: 1842] loss: 0.295775443315506\n",
      "[step: 1843] loss: 0.29573312401771545\n",
      "[step: 1844] loss: 0.2956923246383667\n",
      "[step: 1845] loss: 0.2956516146659851\n",
      "[step: 1846] loss: 0.2956094741821289\n",
      "[step: 1847] loss: 0.29556751251220703\n",
      "[step: 1848] loss: 0.29552626609802246\n",
      "[step: 1849] loss: 0.2954857349395752\n",
      "[step: 1850] loss: 0.29544496536254883\n",
      "[step: 1851] loss: 0.2954034209251404\n",
      "[step: 1852] loss: 0.29536229372024536\n",
      "[step: 1853] loss: 0.2953217625617981\n",
      "[step: 1854] loss: 0.29528123140335083\n",
      "[step: 1855] loss: 0.29524073004722595\n",
      "[step: 1856] loss: 0.29519981145858765\n",
      "[step: 1857] loss: 0.2951592206954956\n",
      "[step: 1858] loss: 0.2951189875602722\n",
      "[step: 1859] loss: 0.29507899284362793\n",
      "[step: 1860] loss: 0.2950385808944702\n",
      "[step: 1861] loss: 0.2949984669685364\n",
      "[step: 1862] loss: 0.29495805501937866\n",
      "[step: 1863] loss: 0.29491835832595825\n",
      "[step: 1864] loss: 0.29487845301628113\n",
      "[step: 1865] loss: 0.29483866691589355\n",
      "[step: 1866] loss: 0.2947988510131836\n",
      "[step: 1867] loss: 0.2947593033313751\n",
      "[step: 1868] loss: 0.2947196364402771\n",
      "[step: 1869] loss: 0.2946801483631134\n",
      "[step: 1870] loss: 0.29464077949523926\n",
      "[step: 1871] loss: 0.2946012318134308\n",
      "[step: 1872] loss: 0.2945619821548462\n",
      "[step: 1873] loss: 0.2945229411125183\n",
      "[step: 1874] loss: 0.2944837808609009\n",
      "[step: 1875] loss: 0.29444456100463867\n",
      "[step: 1876] loss: 0.2944054901599884\n",
      "[step: 1877] loss: 0.29436665773391724\n",
      "[step: 1878] loss: 0.2943277359008789\n",
      "[step: 1879] loss: 0.2942890524864197\n",
      "[step: 1880] loss: 0.29425039887428284\n",
      "[step: 1881] loss: 0.294211745262146\n",
      "[step: 1882] loss: 0.294173002243042\n",
      "[step: 1883] loss: 0.29413461685180664\n",
      "[step: 1884] loss: 0.2940959930419922\n",
      "[step: 1885] loss: 0.2940577268600464\n",
      "[step: 1886] loss: 0.2940194606781006\n",
      "[step: 1887] loss: 0.2939812242984772\n",
      "[step: 1888] loss: 0.2939430773258209\n",
      "[step: 1889] loss: 0.293905109167099\n",
      "[step: 1890] loss: 0.2938671112060547\n",
      "[step: 1891] loss: 0.29382914304733276\n",
      "[step: 1892] loss: 0.2937910556793213\n",
      "[step: 1893] loss: 0.2937535047531128\n",
      "[step: 1894] loss: 0.2937157154083252\n",
      "[step: 1895] loss: 0.29367804527282715\n",
      "[step: 1896] loss: 0.29364025592803955\n",
      "[step: 1897] loss: 0.29360276460647583\n",
      "[step: 1898] loss: 0.29356515407562256\n",
      "[step: 1899] loss: 0.2935279309749603\n",
      "[step: 1900] loss: 0.29349055886268616\n",
      "[step: 1901] loss: 0.2934533953666687\n",
      "[step: 1902] loss: 0.293415904045105\n",
      "[step: 1903] loss: 0.29337888956069946\n",
      "[step: 1904] loss: 0.2933417856693268\n",
      "[step: 1905] loss: 0.2933046519756317\n",
      "[step: 1906] loss: 0.2932676672935486\n",
      "[step: 1907] loss: 0.29323071241378784\n",
      "[step: 1908] loss: 0.29319387674331665\n",
      "[step: 1909] loss: 0.29315710067749023\n",
      "[step: 1910] loss: 0.2931205630302429\n",
      "[step: 1911] loss: 0.2930838167667389\n",
      "[step: 1912] loss: 0.2930472791194916\n",
      "[step: 1913] loss: 0.2930107116699219\n",
      "[step: 1914] loss: 0.2929743528366089\n",
      "[step: 1915] loss: 0.29293784499168396\n",
      "[step: 1916] loss: 0.2929013967514038\n",
      "[step: 1917] loss: 0.2928652763366699\n",
      "[step: 1918] loss: 0.29282885789871216\n",
      "[step: 1919] loss: 0.2927928566932678\n",
      "[step: 1920] loss: 0.29275673627853394\n",
      "[step: 1921] loss: 0.2927205562591553\n",
      "[step: 1922] loss: 0.2926846146583557\n",
      "[step: 1923] loss: 0.2926484942436218\n",
      "[step: 1924] loss: 0.2926127314567566\n",
      "[step: 1925] loss: 0.2925770878791809\n",
      "[step: 1926] loss: 0.29254114627838135\n",
      "[step: 1927] loss: 0.2925054430961609\n",
      "[step: 1928] loss: 0.29246985912323\n",
      "[step: 1929] loss: 0.2924343943595886\n",
      "[step: 1930] loss: 0.29239895939826965\n",
      "[step: 1931] loss: 0.2923632264137268\n",
      "[step: 1932] loss: 0.29232800006866455\n",
      "[step: 1933] loss: 0.29229259490966797\n",
      "[step: 1934] loss: 0.29225724935531616\n",
      "[step: 1935] loss: 0.29222220182418823\n",
      "[step: 1936] loss: 0.29218679666519165\n",
      "[step: 1937] loss: 0.29215162992477417\n",
      "[step: 1938] loss: 0.29211661219596863\n",
      "[step: 1939] loss: 0.29208171367645264\n",
      "[step: 1940] loss: 0.29204678535461426\n",
      "[step: 1941] loss: 0.2920117974281311\n",
      "[step: 1942] loss: 0.2919769883155823\n",
      "[step: 1943] loss: 0.29194211959838867\n",
      "[step: 1944] loss: 0.29190748929977417\n",
      "[step: 1945] loss: 0.2918728291988373\n",
      "[step: 1946] loss: 0.29183822870254517\n",
      "[step: 1947] loss: 0.2918035686016083\n",
      "[step: 1948] loss: 0.29176920652389526\n",
      "[step: 1949] loss: 0.2917344570159912\n",
      "[step: 1950] loss: 0.29170018434524536\n",
      "[step: 1951] loss: 0.29166561365127563\n",
      "[step: 1952] loss: 0.2916315793991089\n",
      "[step: 1953] loss: 0.2915971875190735\n",
      "[step: 1954] loss: 0.2915630340576172\n",
      "[step: 1955] loss: 0.2915290296077728\n",
      "[step: 1956] loss: 0.2914949357509613\n",
      "[step: 1957] loss: 0.29146087169647217\n",
      "[step: 1958] loss: 0.29142695665359497\n",
      "[step: 1959] loss: 0.2913926839828491\n",
      "[step: 1960] loss: 0.291359007358551\n",
      "[step: 1961] loss: 0.2913251519203186\n",
      "[step: 1962] loss: 0.29129141569137573\n",
      "[step: 1963] loss: 0.29125773906707764\n",
      "[step: 1964] loss: 0.29122406244277954\n",
      "[step: 1965] loss: 0.2911904752254486\n",
      "[step: 1966] loss: 0.2911567687988281\n",
      "[step: 1967] loss: 0.2911234498023987\n",
      "[step: 1968] loss: 0.29109007120132446\n",
      "[step: 1969] loss: 0.291056364774704\n",
      "[step: 1970] loss: 0.2910230755805969\n",
      "[step: 1971] loss: 0.29098984599113464\n",
      "[step: 1972] loss: 0.2909564971923828\n",
      "[step: 1973] loss: 0.29092347621917725\n",
      "[step: 1974] loss: 0.29089024662971497\n",
      "[step: 1975] loss: 0.29085710644721985\n",
      "[step: 1976] loss: 0.29082411527633667\n",
      "[step: 1977] loss: 0.29079121351242065\n",
      "[step: 1978] loss: 0.2907581925392151\n",
      "[step: 1979] loss: 0.2907254099845886\n",
      "[step: 1980] loss: 0.2906927466392517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1981] loss: 0.2906603515148163\n",
      "[step: 1982] loss: 0.2906283736228943\n",
      "[step: 1983] loss: 0.29059672355651855\n",
      "[step: 1984] loss: 0.2905663847923279\n",
      "[step: 1985] loss: 0.2905375361442566\n",
      "[step: 1986] loss: 0.2905121445655823\n",
      "[step: 1987] loss: 0.29049310088157654\n",
      "[step: 1988] loss: 0.29048579931259155\n",
      "[step: 1989] loss: 0.2905008792877197\n",
      "[step: 1990] loss: 0.29055941104888916\n",
      "[step: 1991] loss: 0.29070380330085754\n",
      "[step: 1992] loss: 0.2910149395465851\n",
      "[step: 1993] loss: 0.2916662395000458\n",
      "[step: 1994] loss: 0.29298487305641174\n",
      "[step: 1995] loss: 0.2957109212875366\n",
      "[step: 1996] loss: 0.3011888861656189\n",
      "[step: 1997] loss: 0.3127422332763672\n",
      "[step: 1998] loss: 0.3357703685760498\n",
      "[step: 1999] loss: 0.3857972323894501\n",
      "[step: 2000] loss: 0.4811834692955017\n",
      "[step: 2001] loss: 0.6923300623893738\n",
      "[step: 2002] loss: 1.0282517671585083\n",
      "[step: 2003] loss: 1.7121777534484863\n",
      "[step: 2004] loss: 2.1509082317352295\n",
      "[step: 2005] loss: 2.5100221633911133\n",
      "[step: 2006] loss: 1.251800537109375\n",
      "[step: 2007] loss: 0.35412275791168213\n",
      "[step: 2008] loss: 0.7412670850753784\n",
      "[step: 2009] loss: 1.1254353523254395\n",
      "[step: 2010] loss: 0.6363931894302368\n",
      "[step: 2011] loss: 0.39429691433906555\n",
      "[step: 2012] loss: 0.8409560918807983\n",
      "[step: 2013] loss: 0.6608618497848511\n",
      "[step: 2014] loss: 0.34498265385627747\n",
      "[step: 2015] loss: 0.7072935104370117\n",
      "[step: 2016] loss: 0.6044026017189026\n",
      "[step: 2017] loss: 0.33138495683670044\n",
      "[step: 2018] loss: 0.6362212300300598\n",
      "[step: 2019] loss: 0.5676141977310181\n",
      "[step: 2020] loss: 0.33261507749557495\n",
      "[step: 2021] loss: 0.5797575116157532\n",
      "[step: 2022] loss: 0.5258148908615112\n",
      "[step: 2023] loss: 0.3310188055038452\n",
      "[step: 2024] loss: 0.5312303304672241\n",
      "[step: 2025] loss: 0.46827027201652527\n",
      "[step: 2026] loss: 0.3248823285102844\n",
      "[step: 2027] loss: 0.48881396651268005\n",
      "[step: 2028] loss: 0.4141324758529663\n",
      "[step: 2029] loss: 0.3239319920539856\n",
      "[step: 2030] loss: 0.45342010259628296\n",
      "[step: 2031] loss: 0.3722720444202423\n",
      "[step: 2032] loss: 0.3262765109539032\n",
      "[step: 2033] loss: 0.42144861817359924\n",
      "[step: 2034] loss: 0.3434945344924927\n",
      "[step: 2035] loss: 0.3238471746444702\n",
      "[step: 2036] loss: 0.3909880518913269\n",
      "[step: 2037] loss: 0.32715827226638794\n",
      "[step: 2038] loss: 0.3152344226837158\n",
      "[step: 2039] loss: 0.36646395921707153\n",
      "[step: 2040] loss: 0.3216578960418701\n",
      "[step: 2041] loss: 0.3061884343624115\n",
      "[step: 2042] loss: 0.34836944937705994\n",
      "[step: 2043] loss: 0.3224898874759674\n",
      "[step: 2044] loss: 0.3001368045806885\n",
      "[step: 2045] loss: 0.3324950635433197\n",
      "[step: 2046] loss: 0.3234485983848572\n",
      "[step: 2047] loss: 0.29742538928985596\n",
      "[step: 2048] loss: 0.3179510831832886\n",
      "[step: 2049] loss: 0.321313738822937\n",
      "[step: 2050] loss: 0.297908216714859\n",
      "[step: 2051] loss: 0.30744922161102295\n",
      "[step: 2052] loss: 0.31719011068344116\n",
      "[step: 2053] loss: 0.2999219000339508\n",
      "[step: 2054] loss: 0.30080556869506836\n",
      "[step: 2055] loss: 0.31188005208969116\n",
      "[step: 2056] loss: 0.3016883432865143\n",
      "[step: 2057] loss: 0.2968362271785736\n",
      "[step: 2058] loss: 0.30583837628364563\n",
      "[step: 2059] loss: 0.3023568093776703\n",
      "[step: 2060] loss: 0.29533448815345764\n",
      "[step: 2061] loss: 0.30044659972190857\n",
      "[step: 2062] loss: 0.30190780758857727\n",
      "[step: 2063] loss: 0.29579147696495056\n",
      "[step: 2064] loss: 0.29661834239959717\n",
      "[step: 2065] loss: 0.3001728355884552\n",
      "[step: 2066] loss: 0.29684412479400635\n",
      "[step: 2067] loss: 0.29465532302856445\n",
      "[step: 2068] loss: 0.29760193824768066\n",
      "[step: 2069] loss: 0.2972233295440674\n",
      "[step: 2070] loss: 0.29427218437194824\n",
      "[step: 2071] loss: 0.29533419013023376\n",
      "[step: 2072] loss: 0.2966654896736145\n",
      "[step: 2073] loss: 0.294659823179245\n",
      "[step: 2074] loss: 0.29403412342071533\n",
      "[step: 2075] loss: 0.2955062985420227\n",
      "[step: 2076] loss: 0.2949499487876892\n",
      "[step: 2077] loss: 0.2936251759529114\n",
      "[step: 2078] loss: 0.29427698254585266\n",
      "[step: 2079] loss: 0.2947400212287903\n",
      "[step: 2080] loss: 0.2937105596065521\n",
      "[step: 2081] loss: 0.29344600439071655\n",
      "[step: 2082] loss: 0.29412171244621277\n",
      "[step: 2083] loss: 0.2938472628593445\n",
      "[step: 2084] loss: 0.2931686341762543\n",
      "[step: 2085] loss: 0.29342198371887207\n",
      "[step: 2086] loss: 0.2936908006668091\n",
      "[step: 2087] loss: 0.2932039499282837\n",
      "[step: 2088] loss: 0.2929564118385315\n",
      "[step: 2089] loss: 0.2932487726211548\n",
      "[step: 2090] loss: 0.29319000244140625\n",
      "[step: 2091] loss: 0.2928062677383423\n",
      "[step: 2092] loss: 0.2928037643432617\n",
      "[step: 2093] loss: 0.2929677367210388\n",
      "[step: 2094] loss: 0.2927868962287903\n",
      "[step: 2095] loss: 0.2925640940666199\n",
      "[step: 2096] loss: 0.2926357388496399\n",
      "[step: 2097] loss: 0.29267430305480957\n",
      "[step: 2098] loss: 0.2924845814704895\n",
      "[step: 2099] loss: 0.2923741936683655\n",
      "[step: 2100] loss: 0.29243379831314087\n",
      "[step: 2101] loss: 0.2923988699913025\n",
      "[step: 2102] loss: 0.29224729537963867\n",
      "[step: 2103] loss: 0.29219484329223633\n",
      "[step: 2104] loss: 0.2922239601612091\n",
      "[step: 2105] loss: 0.29216328263282776\n",
      "[step: 2106] loss: 0.2920507788658142\n",
      "[step: 2107] loss: 0.2920191287994385\n",
      "[step: 2108] loss: 0.29202157258987427\n",
      "[step: 2109] loss: 0.29195576906204224\n",
      "[step: 2110] loss: 0.2918704152107239\n",
      "[step: 2111] loss: 0.29184257984161377\n",
      "[step: 2112] loss: 0.2918277978897095\n",
      "[step: 2113] loss: 0.29176679253578186\n",
      "[step: 2114] loss: 0.29169878363609314\n",
      "[step: 2115] loss: 0.29166969656944275\n",
      "[step: 2116] loss: 0.29164543747901917\n",
      "[step: 2117] loss: 0.2915905714035034\n",
      "[step: 2118] loss: 0.2915326654911041\n",
      "[step: 2119] loss: 0.2915000319480896\n",
      "[step: 2120] loss: 0.29147034883499146\n",
      "[step: 2121] loss: 0.2914208769798279\n",
      "[step: 2122] loss: 0.2913689613342285\n",
      "[step: 2123] loss: 0.2913336157798767\n",
      "[step: 2124] loss: 0.2913011610507965\n",
      "[step: 2125] loss: 0.2912563383579254\n",
      "[step: 2126] loss: 0.29120850563049316\n",
      "[step: 2127] loss: 0.29117050766944885\n",
      "[step: 2128] loss: 0.29113680124282837\n",
      "[step: 2129] loss: 0.2910950779914856\n",
      "[step: 2130] loss: 0.2910500168800354\n",
      "[step: 2131] loss: 0.29101067781448364\n",
      "[step: 2132] loss: 0.2909754514694214\n",
      "[step: 2133] loss: 0.2909359931945801\n",
      "[step: 2134] loss: 0.2908930778503418\n",
      "[step: 2135] loss: 0.29085320234298706\n",
      "[step: 2136] loss: 0.29081690311431885\n",
      "[step: 2137] loss: 0.2907789647579193\n",
      "[step: 2138] loss: 0.2907381057739258\n",
      "[step: 2139] loss: 0.2906982898712158\n",
      "[step: 2140] loss: 0.2906609773635864\n",
      "[step: 2141] loss: 0.2906237840652466\n",
      "[step: 2142] loss: 0.2905843257904053\n",
      "[step: 2143] loss: 0.29054495692253113\n",
      "[step: 2144] loss: 0.29050707817077637\n",
      "[step: 2145] loss: 0.2904701232910156\n",
      "[step: 2146] loss: 0.2904322147369385\n",
      "[step: 2147] loss: 0.2903933525085449\n",
      "[step: 2148] loss: 0.29035529494285583\n",
      "[step: 2149] loss: 0.29031819105148315\n",
      "[step: 2150] loss: 0.2902810275554657\n",
      "[step: 2151] loss: 0.2902431786060333\n",
      "[step: 2152] loss: 0.2902049720287323\n",
      "[step: 2153] loss: 0.2901678681373596\n",
      "[step: 2154] loss: 0.2901310920715332\n",
      "[step: 2155] loss: 0.29009389877319336\n",
      "[step: 2156] loss: 0.29005634784698486\n",
      "[step: 2157] loss: 0.290019154548645\n",
      "[step: 2158] loss: 0.28998255729675293\n",
      "[step: 2159] loss: 0.28994572162628174\n",
      "[step: 2160] loss: 0.2899090051651001\n",
      "[step: 2161] loss: 0.2898719310760498\n",
      "[step: 2162] loss: 0.28983524441719055\n",
      "[step: 2163] loss: 0.289798766374588\n",
      "[step: 2164] loss: 0.28976237773895264\n",
      "[step: 2165] loss: 0.28972581028938293\n",
      "[step: 2166] loss: 0.289689302444458\n",
      "[step: 2167] loss: 0.2896530032157898\n",
      "[step: 2168] loss: 0.28961679339408875\n",
      "[step: 2169] loss: 0.2895810008049011\n",
      "[step: 2170] loss: 0.2895446717739105\n",
      "[step: 2171] loss: 0.2895086109638214\n",
      "[step: 2172] loss: 0.289472758769989\n",
      "[step: 2173] loss: 0.289436936378479\n",
      "[step: 2174] loss: 0.289401113986969\n",
      "[step: 2175] loss: 0.2893654704093933\n",
      "[step: 2176] loss: 0.2893297076225281\n",
      "[step: 2177] loss: 0.28929412364959717\n",
      "[step: 2178] loss: 0.2892586290836334\n",
      "[step: 2179] loss: 0.28922319412231445\n",
      "[step: 2180] loss: 0.28918778896331787\n",
      "[step: 2181] loss: 0.28915244340896606\n",
      "[step: 2182] loss: 0.2891172766685486\n",
      "[step: 2183] loss: 0.2890821695327759\n",
      "[step: 2184] loss: 0.2890469431877136\n",
      "[step: 2185] loss: 0.28901174664497375\n",
      "[step: 2186] loss: 0.288976788520813\n",
      "[step: 2187] loss: 0.2889419198036194\n",
      "[step: 2188] loss: 0.2889070510864258\n",
      "[step: 2189] loss: 0.2888721823692322\n",
      "[step: 2190] loss: 0.2888375520706177\n",
      "[step: 2191] loss: 0.28880301117897034\n",
      "[step: 2192] loss: 0.28876829147338867\n",
      "[step: 2193] loss: 0.2887338399887085\n",
      "[step: 2194] loss: 0.2886993885040283\n",
      "[step: 2195] loss: 0.28866487741470337\n",
      "[step: 2196] loss: 0.2886304557323456\n",
      "[step: 2197] loss: 0.28859618306159973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2198] loss: 0.288562148809433\n",
      "[step: 2199] loss: 0.28852781653404236\n",
      "[step: 2200] loss: 0.2884936034679413\n",
      "[step: 2201] loss: 0.2884596586227417\n",
      "[step: 2202] loss: 0.2884256839752197\n",
      "[step: 2203] loss: 0.28839176893234253\n",
      "[step: 2204] loss: 0.28835779428482056\n",
      "[step: 2205] loss: 0.2883240878582001\n",
      "[step: 2206] loss: 0.2882905602455139\n",
      "[step: 2207] loss: 0.2882564067840576\n",
      "[step: 2208] loss: 0.2882229685783386\n",
      "[step: 2209] loss: 0.28818953037261963\n",
      "[step: 2210] loss: 0.2881559431552887\n",
      "[step: 2211] loss: 0.28812241554260254\n",
      "[step: 2212] loss: 0.28808921575546265\n",
      "[step: 2213] loss: 0.2880558967590332\n",
      "[step: 2214] loss: 0.28802257776260376\n",
      "[step: 2215] loss: 0.2879894971847534\n",
      "[step: 2216] loss: 0.287956178188324\n",
      "[step: 2217] loss: 0.2879232168197632\n",
      "[step: 2218] loss: 0.2878900170326233\n",
      "[step: 2219] loss: 0.2878572642803192\n",
      "[step: 2220] loss: 0.28782421350479126\n",
      "[step: 2221] loss: 0.28779128193855286\n",
      "[step: 2222] loss: 0.28775832056999207\n",
      "[step: 2223] loss: 0.2877255082130432\n",
      "[step: 2224] loss: 0.28769299387931824\n",
      "[step: 2225] loss: 0.2876604199409485\n",
      "[step: 2226] loss: 0.28762781620025635\n",
      "[step: 2227] loss: 0.2875952124595642\n",
      "[step: 2228] loss: 0.2875627279281616\n",
      "[step: 2229] loss: 0.2875302731990814\n",
      "[step: 2230] loss: 0.2874979078769684\n",
      "[step: 2231] loss: 0.28746557235717773\n",
      "[step: 2232] loss: 0.287433385848999\n",
      "[step: 2233] loss: 0.2874011993408203\n",
      "[step: 2234] loss: 0.2873690724372864\n",
      "[step: 2235] loss: 0.2873368263244629\n",
      "[step: 2236] loss: 0.28730469942092896\n",
      "[step: 2237] loss: 0.28727293014526367\n",
      "[step: 2238] loss: 0.2872408628463745\n",
      "[step: 2239] loss: 0.28720903396606445\n",
      "[step: 2240] loss: 0.28717705607414246\n",
      "[step: 2241] loss: 0.2871452867984772\n",
      "[step: 2242] loss: 0.28711357712745667\n",
      "[step: 2243] loss: 0.287081778049469\n",
      "[step: 2244] loss: 0.2870503067970276\n",
      "[step: 2245] loss: 0.2870185077190399\n",
      "[step: 2246] loss: 0.2869870066642761\n",
      "[step: 2247] loss: 0.2869558334350586\n",
      "[step: 2248] loss: 0.28692418336868286\n",
      "[step: 2249] loss: 0.28689295053482056\n",
      "[step: 2250] loss: 0.28686147928237915\n",
      "[step: 2251] loss: 0.2868301272392273\n",
      "[step: 2252] loss: 0.28679901361465454\n",
      "[step: 2253] loss: 0.28676778078079224\n",
      "[step: 2254] loss: 0.28673672676086426\n",
      "[step: 2255] loss: 0.2867056727409363\n",
      "[step: 2256] loss: 0.286674439907074\n",
      "[step: 2257] loss: 0.2866436243057251\n",
      "[step: 2258] loss: 0.28661268949508667\n",
      "[step: 2259] loss: 0.2865816652774811\n",
      "[step: 2260] loss: 0.28655093908309937\n",
      "[step: 2261] loss: 0.28652024269104004\n",
      "[step: 2262] loss: 0.286489337682724\n",
      "[step: 2263] loss: 0.2864587903022766\n",
      "[step: 2264] loss: 0.2864281237125397\n",
      "[step: 2265] loss: 0.28639739751815796\n",
      "[step: 2266] loss: 0.28636693954467773\n",
      "[step: 2267] loss: 0.28633642196655273\n",
      "[step: 2268] loss: 0.2863059341907501\n",
      "[step: 2269] loss: 0.2862755060195923\n",
      "[step: 2270] loss: 0.2862451672554016\n",
      "[step: 2271] loss: 0.28621476888656616\n",
      "[step: 2272] loss: 0.28618454933166504\n",
      "[step: 2273] loss: 0.2861543893814087\n",
      "[step: 2274] loss: 0.2861240804195404\n",
      "[step: 2275] loss: 0.28609412908554077\n",
      "[step: 2276] loss: 0.2860640287399292\n",
      "[step: 2277] loss: 0.28603395819664\n",
      "[step: 2278] loss: 0.2860039472579956\n",
      "[step: 2279] loss: 0.28597402572631836\n",
      "[step: 2280] loss: 0.28594422340393066\n",
      "[step: 2281] loss: 0.2859143018722534\n",
      "[step: 2282] loss: 0.2858845591545105\n",
      "[step: 2283] loss: 0.2858548164367676\n",
      "[step: 2284] loss: 0.2858249247074127\n",
      "[step: 2285] loss: 0.2857953906059265\n",
      "[step: 2286] loss: 0.28576570749282837\n",
      "[step: 2287] loss: 0.2857363224029541\n",
      "[step: 2288] loss: 0.28570663928985596\n",
      "[step: 2289] loss: 0.285677045583725\n",
      "[step: 2290] loss: 0.28564774990081787\n",
      "[step: 2291] loss: 0.28561827540397644\n",
      "[step: 2292] loss: 0.2855890393257141\n",
      "[step: 2293] loss: 0.28555989265441895\n",
      "[step: 2294] loss: 0.28553059697151184\n",
      "[step: 2295] loss: 0.28550177812576294\n",
      "[step: 2296] loss: 0.2854728698730469\n",
      "[step: 2297] loss: 0.28544431924819946\n",
      "[step: 2298] loss: 0.28541624546051025\n",
      "[step: 2299] loss: 0.28538915514945984\n",
      "[step: 2300] loss: 0.28536325693130493\n",
      "[step: 2301] loss: 0.2853403091430664\n",
      "[step: 2302] loss: 0.2853218913078308\n",
      "[step: 2303] loss: 0.28531232476234436\n",
      "[step: 2304] loss: 0.28531888127326965\n",
      "[step: 2305] loss: 0.28535664081573486\n",
      "[step: 2306] loss: 0.2854526937007904\n",
      "[step: 2307] loss: 0.2856626510620117\n",
      "[step: 2308] loss: 0.2860904037952423\n",
      "[step: 2309] loss: 0.28695595264434814\n",
      "[step: 2310] loss: 0.2886618673801422\n",
      "[step: 2311] loss: 0.2921205759048462\n",
      "[step: 2312] loss: 0.29891064763069153\n",
      "[step: 2313] loss: 0.31299567222595215\n",
      "[step: 2314] loss: 0.34032756090164185\n",
      "[step: 2315] loss: 0.3986731767654419\n",
      "[step: 2316] loss: 0.5057291388511658\n",
      "[step: 2317] loss: 0.7367596626281738\n",
      "[step: 2318] loss: 1.0774444341659546\n",
      "[step: 2319] loss: 1.731471300125122\n",
      "[step: 2320] loss: 2.0220789909362793\n",
      "[step: 2321] loss: 2.1066813468933105\n",
      "[step: 2322] loss: 0.9386528730392456\n",
      "[step: 2323] loss: 0.3232421875\n",
      "[step: 2324] loss: 0.8111095428466797\n",
      "[step: 2325] loss: 1.0083892345428467\n",
      "[step: 2326] loss: 0.5137443542480469\n",
      "[step: 2327] loss: 0.40095770359039307\n",
      "[step: 2328] loss: 0.763434648513794\n",
      "[step: 2329] loss: 0.5778807401657104\n",
      "[step: 2330] loss: 0.3302440345287323\n",
      "[step: 2331] loss: 0.623900830745697\n",
      "[step: 2332] loss: 0.5709644556045532\n",
      "[step: 2333] loss: 0.31672459840774536\n",
      "[step: 2334] loss: 0.5402998924255371\n",
      "[step: 2335] loss: 0.5631241798400879\n",
      "[step: 2336] loss: 0.3228026032447815\n",
      "[step: 2337] loss: 0.4732629060745239\n",
      "[step: 2338] loss: 0.5238388776779175\n",
      "[step: 2339] loss: 0.3211597204208374\n",
      "[step: 2340] loss: 0.4264565706253052\n",
      "[step: 2341] loss: 0.4717150032520294\n",
      "[step: 2342] loss: 0.3111012876033783\n",
      "[step: 2343] loss: 0.3954569101333618\n",
      "[step: 2344] loss: 0.4291883707046509\n",
      "[step: 2345] loss: 0.30521589517593384\n",
      "[step: 2346] loss: 0.37567663192749023\n",
      "[step: 2347] loss: 0.39680469036102295\n",
      "[step: 2348] loss: 0.3012905716896057\n",
      "[step: 2349] loss: 0.35801762342453003\n",
      "[step: 2350] loss: 0.3736010789871216\n",
      "[step: 2351] loss: 0.2972451448440552\n",
      "[step: 2352] loss: 0.3367008566856384\n",
      "[step: 2353] loss: 0.3591277599334717\n",
      "[step: 2354] loss: 0.2979571223258972\n",
      "[step: 2355] loss: 0.3161165714263916\n",
      "[step: 2356] loss: 0.3473801016807556\n",
      "[step: 2357] loss: 0.30427199602127075\n",
      "[step: 2358] loss: 0.3003320097923279\n",
      "[step: 2359] loss: 0.3319845199584961\n",
      "[step: 2360] loss: 0.31050723791122437\n",
      "[step: 2361] loss: 0.29190969467163086\n",
      "[step: 2362] loss: 0.3138737678527832\n",
      "[step: 2363] loss: 0.31184929609298706\n",
      "[step: 2364] loss: 0.29152774810791016\n",
      "[step: 2365] loss: 0.3003349304199219\n",
      "[step: 2366] loss: 0.30890265107154846\n",
      "[step: 2367] loss: 0.2948008179664612\n",
      "[step: 2368] loss: 0.2927621901035309\n",
      "[step: 2369] loss: 0.3031808137893677\n",
      "[step: 2370] loss: 0.29758548736572266\n",
      "[step: 2371] loss: 0.28979843854904175\n",
      "[step: 2372] loss: 0.29646921157836914\n",
      "[step: 2373] loss: 0.2980249524116516\n",
      "[step: 2374] loss: 0.2902674376964569\n",
      "[step: 2375] loss: 0.2912839651107788\n",
      "[step: 2376] loss: 0.2960011065006256\n",
      "[step: 2377] loss: 0.2920457720756531\n",
      "[step: 2378] loss: 0.28891944885253906\n",
      "[step: 2379] loss: 0.29243195056915283\n",
      "[step: 2380] loss: 0.2926541864871979\n",
      "[step: 2381] loss: 0.28892892599105835\n",
      "[step: 2382] loss: 0.28940755128860474\n",
      "[step: 2383] loss: 0.291553795337677\n",
      "[step: 2384] loss: 0.28976428508758545\n",
      "[step: 2385] loss: 0.2881496548652649\n",
      "[step: 2386] loss: 0.2897489666938782\n",
      "[step: 2387] loss: 0.2900623679161072\n",
      "[step: 2388] loss: 0.28824904561042786\n",
      "[step: 2389] loss: 0.2882848083972931\n",
      "[step: 2390] loss: 0.289397656917572\n",
      "[step: 2391] loss: 0.288651704788208\n",
      "[step: 2392] loss: 0.2876834273338318\n",
      "[step: 2393] loss: 0.28830254077911377\n",
      "[step: 2394] loss: 0.28863799571990967\n",
      "[step: 2395] loss: 0.28780290484428406\n",
      "[step: 2396] loss: 0.28753817081451416\n",
      "[step: 2397] loss: 0.2880861163139343\n",
      "[step: 2398] loss: 0.2879743278026581\n",
      "[step: 2399] loss: 0.28736984729766846\n",
      "[step: 2400] loss: 0.28742414712905884\n",
      "[step: 2401] loss: 0.2877315282821655\n",
      "[step: 2402] loss: 0.2874557673931122\n",
      "[step: 2403] loss: 0.2871050238609314\n",
      "[step: 2404] loss: 0.28724658489227295\n",
      "[step: 2405] loss: 0.2873649597167969\n",
      "[step: 2406] loss: 0.28709882497787476\n",
      "[step: 2407] loss: 0.2869197726249695\n",
      "[step: 2408] loss: 0.2870432734489441\n",
      "[step: 2409] loss: 0.2870551645755768\n",
      "[step: 2410] loss: 0.28683948516845703\n",
      "[step: 2411] loss: 0.28674644231796265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2412] loss: 0.28682541847229004\n",
      "[step: 2413] loss: 0.2867906391620636\n",
      "[step: 2414] loss: 0.28663158416748047\n",
      "[step: 2415] loss: 0.2865767478942871\n",
      "[step: 2416] loss: 0.286618709564209\n",
      "[step: 2417] loss: 0.2865734100341797\n",
      "[step: 2418] loss: 0.28645551204681396\n",
      "[step: 2419] loss: 0.28641223907470703\n",
      "[step: 2420] loss: 0.2864267826080322\n",
      "[step: 2421] loss: 0.286381334066391\n",
      "[step: 2422] loss: 0.28629082441329956\n",
      "[step: 2423] loss: 0.28624942898750305\n",
      "[step: 2424] loss: 0.28624647855758667\n",
      "[step: 2425] loss: 0.2862062454223633\n",
      "[step: 2426] loss: 0.2861344516277313\n",
      "[step: 2427] loss: 0.2860923409461975\n",
      "[step: 2428] loss: 0.2860780656337738\n",
      "[step: 2429] loss: 0.28604191541671753\n",
      "[step: 2430] loss: 0.2859826982021332\n",
      "[step: 2431] loss: 0.2859390079975128\n",
      "[step: 2432] loss: 0.28591620922088623\n",
      "[step: 2433] loss: 0.28588351607322693\n",
      "[step: 2434] loss: 0.28583335876464844\n",
      "[step: 2435] loss: 0.28578925132751465\n",
      "[step: 2436] loss: 0.2857609689235687\n",
      "[step: 2437] loss: 0.285730242729187\n",
      "[step: 2438] loss: 0.2856866121292114\n",
      "[step: 2439] loss: 0.2856435775756836\n",
      "[step: 2440] loss: 0.28561076521873474\n",
      "[step: 2441] loss: 0.2855799198150635\n",
      "[step: 2442] loss: 0.28554123640060425\n",
      "[step: 2443] loss: 0.2854999303817749\n",
      "[step: 2444] loss: 0.2854643166065216\n",
      "[step: 2445] loss: 0.2854325473308563\n",
      "[step: 2446] loss: 0.28539732098579407\n",
      "[step: 2447] loss: 0.2853584587574005\n",
      "[step: 2448] loss: 0.2853214144706726\n",
      "[step: 2449] loss: 0.28528842329978943\n",
      "[step: 2450] loss: 0.28525465726852417\n",
      "[step: 2451] loss: 0.28521788120269775\n",
      "[step: 2452] loss: 0.28518107533454895\n",
      "[step: 2453] loss: 0.2851465940475464\n",
      "[step: 2454] loss: 0.28511321544647217\n",
      "[step: 2455] loss: 0.2850785553455353\n",
      "[step: 2456] loss: 0.285042405128479\n",
      "[step: 2457] loss: 0.2850072979927063\n",
      "[step: 2458] loss: 0.2849735617637634\n",
      "[step: 2459] loss: 0.28493988513946533\n",
      "[step: 2460] loss: 0.2849048674106598\n",
      "[step: 2461] loss: 0.2848696708679199\n",
      "[step: 2462] loss: 0.2848355770111084\n",
      "[step: 2463] loss: 0.284802109003067\n",
      "[step: 2464] loss: 0.28476834297180176\n",
      "[step: 2465] loss: 0.28473377227783203\n",
      "[step: 2466] loss: 0.28469955921173096\n",
      "[step: 2467] loss: 0.28466594219207764\n",
      "[step: 2468] loss: 0.2846323251724243\n",
      "[step: 2469] loss: 0.28459882736206055\n",
      "[step: 2470] loss: 0.28456488251686096\n",
      "[step: 2471] loss: 0.28453120589256287\n",
      "[step: 2472] loss: 0.2844978868961334\n",
      "[step: 2473] loss: 0.28446459770202637\n",
      "[step: 2474] loss: 0.2844313383102417\n",
      "[step: 2475] loss: 0.28439801931381226\n",
      "[step: 2476] loss: 0.2843644917011261\n",
      "[step: 2477] loss: 0.28433170914649963\n",
      "[step: 2478] loss: 0.2842985987663269\n",
      "[step: 2479] loss: 0.28426554799079895\n",
      "[step: 2480] loss: 0.284232497215271\n",
      "[step: 2481] loss: 0.28419965505599976\n",
      "[step: 2482] loss: 0.2841668725013733\n",
      "[step: 2483] loss: 0.2841343283653259\n",
      "[step: 2484] loss: 0.284101665019989\n",
      "[step: 2485] loss: 0.28406891226768494\n",
      "[step: 2486] loss: 0.28403645753860474\n",
      "[step: 2487] loss: 0.2840039134025574\n",
      "[step: 2488] loss: 0.2839714288711548\n",
      "[step: 2489] loss: 0.2839392423629761\n",
      "[step: 2490] loss: 0.28390687704086304\n",
      "[step: 2491] loss: 0.2838746905326843\n",
      "[step: 2492] loss: 0.28384262323379517\n",
      "[step: 2493] loss: 0.28381043672561646\n",
      "[step: 2494] loss: 0.28377851843833923\n",
      "[step: 2495] loss: 0.2837466895580292\n",
      "[step: 2496] loss: 0.2837144732475281\n",
      "[step: 2497] loss: 0.28368276357650757\n",
      "[step: 2498] loss: 0.2836509943008423\n",
      "[step: 2499] loss: 0.2836191952228546\n",
      "[step: 2500] loss: 0.28358766436576843\n",
      "[step: 2501] loss: 0.2835560441017151\n",
      "[step: 2502] loss: 0.28352445363998413\n",
      "[step: 2503] loss: 0.2834928631782532\n",
      "[step: 2504] loss: 0.2834616005420685\n",
      "[step: 2505] loss: 0.283430278301239\n",
      "[step: 2506] loss: 0.28339898586273193\n",
      "[step: 2507] loss: 0.2833676338195801\n",
      "[step: 2508] loss: 0.28333649039268494\n",
      "[step: 2509] loss: 0.28330522775650024\n",
      "[step: 2510] loss: 0.28327423334121704\n",
      "[step: 2511] loss: 0.28324323892593384\n",
      "[step: 2512] loss: 0.28321242332458496\n",
      "[step: 2513] loss: 0.28318148851394653\n",
      "[step: 2514] loss: 0.28315046429634094\n",
      "[step: 2515] loss: 0.283119797706604\n",
      "[step: 2516] loss: 0.2830890417098999\n",
      "[step: 2517] loss: 0.2830583453178406\n",
      "[step: 2518] loss: 0.28302767872810364\n",
      "[step: 2519] loss: 0.28299733996391296\n",
      "[step: 2520] loss: 0.28296661376953125\n",
      "[step: 2521] loss: 0.282936155796051\n",
      "[step: 2522] loss: 0.282905638217926\n",
      "[step: 2523] loss: 0.2828754186630249\n",
      "[step: 2524] loss: 0.2828448414802551\n",
      "[step: 2525] loss: 0.28281474113464355\n",
      "[step: 2526] loss: 0.28278470039367676\n",
      "[step: 2527] loss: 0.2827545404434204\n",
      "[step: 2528] loss: 0.282724529504776\n",
      "[step: 2529] loss: 0.28269436955451965\n",
      "[step: 2530] loss: 0.2826644778251648\n",
      "[step: 2531] loss: 0.2826344966888428\n",
      "[step: 2532] loss: 0.2826043367385864\n",
      "[step: 2533] loss: 0.28257471323013306\n",
      "[step: 2534] loss: 0.28254514932632446\n",
      "[step: 2535] loss: 0.28251543641090393\n",
      "[step: 2536] loss: 0.2824857831001282\n",
      "[step: 2537] loss: 0.2824559211730957\n",
      "[step: 2538] loss: 0.28242653608322144\n",
      "[step: 2539] loss: 0.28239694237709045\n",
      "[step: 2540] loss: 0.282367467880249\n",
      "[step: 2541] loss: 0.2823382318019867\n",
      "[step: 2542] loss: 0.2823086977005005\n",
      "[step: 2543] loss: 0.28227949142456055\n",
      "[step: 2544] loss: 0.28225022554397583\n",
      "[step: 2545] loss: 0.2822209298610687\n",
      "[step: 2546] loss: 0.28219181299209595\n",
      "[step: 2547] loss: 0.28216269612312317\n",
      "[step: 2548] loss: 0.28213363885879517\n",
      "[step: 2549] loss: 0.28210464119911194\n",
      "[step: 2550] loss: 0.2820757031440735\n",
      "[step: 2551] loss: 0.28204673528671265\n",
      "[step: 2552] loss: 0.28201794624328613\n",
      "[step: 2553] loss: 0.2819889783859253\n",
      "[step: 2554] loss: 0.28196030855178833\n",
      "[step: 2555] loss: 0.2819315195083618\n",
      "[step: 2556] loss: 0.2819027900695801\n",
      "[step: 2557] loss: 0.2818741500377655\n",
      "[step: 2558] loss: 0.2818456292152405\n",
      "[step: 2559] loss: 0.28181713819503784\n",
      "[step: 2560] loss: 0.28178852796554565\n",
      "[step: 2561] loss: 0.28176015615463257\n",
      "[step: 2562] loss: 0.2817314863204956\n",
      "[step: 2563] loss: 0.28170329332351685\n",
      "[step: 2564] loss: 0.28167515993118286\n",
      "[step: 2565] loss: 0.28164681792259216\n",
      "[step: 2566] loss: 0.28161856532096863\n",
      "[step: 2567] loss: 0.2815902829170227\n",
      "[step: 2568] loss: 0.2815621495246887\n",
      "[step: 2569] loss: 0.2815341651439667\n",
      "[step: 2570] loss: 0.28150632977485657\n",
      "[step: 2571] loss: 0.2814781665802002\n",
      "[step: 2572] loss: 0.2814503014087677\n",
      "[step: 2573] loss: 0.28142249584198\n",
      "[step: 2574] loss: 0.28139448165893555\n",
      "[step: 2575] loss: 0.28136658668518066\n",
      "[step: 2576] loss: 0.28133881092071533\n",
      "[step: 2577] loss: 0.2813110947608948\n",
      "[step: 2578] loss: 0.28128349781036377\n",
      "[step: 2579] loss: 0.2812557816505432\n",
      "[step: 2580] loss: 0.2812281548976898\n",
      "[step: 2581] loss: 0.28120070695877075\n",
      "[step: 2582] loss: 0.28117313981056213\n",
      "[step: 2583] loss: 0.28114572167396545\n",
      "[step: 2584] loss: 0.2811182737350464\n",
      "[step: 2585] loss: 0.28109097480773926\n",
      "[step: 2586] loss: 0.2810634970664978\n",
      "[step: 2587] loss: 0.28103628754615784\n",
      "[step: 2588] loss: 0.2810089588165283\n",
      "[step: 2589] loss: 0.28098195791244507\n",
      "[step: 2590] loss: 0.2809547483921051\n",
      "[step: 2591] loss: 0.2809275984764099\n",
      "[step: 2592] loss: 0.2809004783630371\n",
      "[step: 2593] loss: 0.28087344765663147\n",
      "[step: 2594] loss: 0.2808464765548706\n",
      "[step: 2595] loss: 0.28081950545310974\n",
      "[step: 2596] loss: 0.28079259395599365\n",
      "[step: 2597] loss: 0.2807655334472656\n",
      "[step: 2598] loss: 0.2807387709617615\n",
      "[step: 2599] loss: 0.2807120978832245\n",
      "[step: 2600] loss: 0.2806851863861084\n",
      "[step: 2601] loss: 0.2806586027145386\n",
      "[step: 2602] loss: 0.28063201904296875\n",
      "[step: 2603] loss: 0.280605673789978\n",
      "[step: 2604] loss: 0.28057923913002014\n",
      "[step: 2605] loss: 0.2805531322956085\n",
      "[step: 2606] loss: 0.28052711486816406\n",
      "[step: 2607] loss: 0.2805017828941345\n",
      "[step: 2608] loss: 0.2804771065711975\n",
      "[step: 2609] loss: 0.2804538607597351\n",
      "[step: 2610] loss: 0.28043311834335327\n",
      "[step: 2611] loss: 0.2804167866706848\n",
      "[step: 2612] loss: 0.28040802478790283\n",
      "[step: 2613] loss: 0.2804139256477356\n",
      "[step: 2614] loss: 0.28044572472572327\n",
      "[step: 2615] loss: 0.2805270552635193\n",
      "[step: 2616] loss: 0.2807002067565918\n",
      "[step: 2617] loss: 0.2810512185096741\n",
      "[step: 2618] loss: 0.2817358374595642\n",
      "[step: 2619] loss: 0.28308722376823425\n",
      "[step: 2620] loss: 0.28568899631500244\n",
      "[step: 2621] loss: 0.2908916175365448\n",
      "[step: 2622] loss: 0.3008728623390198\n",
      "[step: 2623] loss: 0.3213476240634918\n",
      "[step: 2624] loss: 0.3598211109638214\n",
      "[step: 2625] loss: 0.44081318378448486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2626] loss: 0.5806134939193726\n",
      "[step: 2627] loss: 0.8720248341560364\n",
      "[step: 2628] loss: 1.234910249710083\n",
      "[step: 2629] loss: 1.8447495698928833\n",
      "[step: 2630] loss: 1.8087588548660278\n",
      "[step: 2631] loss: 1.4101574420928955\n",
      "[step: 2632] loss: 0.4786500334739685\n",
      "[step: 2633] loss: 0.43737465143203735\n",
      "[step: 2634] loss: 1.0021458864212036\n",
      "[step: 2635] loss: 0.7671385407447815\n",
      "[step: 2636] loss: 0.33168601989746094\n",
      "[step: 2637] loss: 0.5336203575134277\n",
      "[step: 2638] loss: 0.6691733598709106\n",
      "[step: 2639] loss: 0.3826967477798462\n",
      "[step: 2640] loss: 0.37222179770469666\n",
      "[step: 2641] loss: 0.5653466582298279\n",
      "[step: 2642] loss: 0.4269915819168091\n",
      "[step: 2643] loss: 0.3195348381996155\n",
      "[step: 2644] loss: 0.48439353704452515\n",
      "[step: 2645] loss: 0.45461952686309814\n",
      "[step: 2646] loss: 0.3100426495075226\n",
      "[step: 2647] loss: 0.4130551815032959\n",
      "[step: 2648] loss: 0.4389648735523224\n",
      "[step: 2649] loss: 0.3070254325866699\n",
      "[step: 2650] loss: 0.3610416650772095\n",
      "[step: 2651] loss: 0.40838950872421265\n",
      "[step: 2652] loss: 0.3068276047706604\n",
      "[step: 2653] loss: 0.32997772097587585\n",
      "[step: 2654] loss: 0.3800947666168213\n",
      "[step: 2655] loss: 0.3077526092529297\n",
      "[step: 2656] loss: 0.3114408552646637\n",
      "[step: 2657] loss: 0.3564240038394928\n",
      "[step: 2658] loss: 0.3083122968673706\n",
      "[step: 2659] loss: 0.29480066895484924\n",
      "[step: 2660] loss: 0.3336281478404999\n",
      "[step: 2661] loss: 0.31230154633522034\n",
      "[step: 2662] loss: 0.28609758615493774\n",
      "[step: 2663] loss: 0.310907781124115\n",
      "[step: 2664] loss: 0.3156052529811859\n",
      "[step: 2665] loss: 0.2900388538837433\n",
      "[step: 2666] loss: 0.2926560044288635\n",
      "[step: 2667] loss: 0.3092806935310364\n",
      "[step: 2668] loss: 0.29843270778656006\n",
      "[step: 2669] loss: 0.2852286100387573\n",
      "[step: 2670] loss: 0.29593563079833984\n",
      "[step: 2671] loss: 0.30051741003990173\n",
      "[step: 2672] loss: 0.28776833415031433\n",
      "[step: 2673] loss: 0.2868421971797943\n",
      "[step: 2674] loss: 0.2957509756088257\n",
      "[step: 2675] loss: 0.29163438081741333\n",
      "[step: 2676] loss: 0.28437912464141846\n",
      "[step: 2677] loss: 0.2885031998157501\n",
      "[step: 2678] loss: 0.2914906144142151\n",
      "[step: 2679] loss: 0.2858774960041046\n",
      "[step: 2680] loss: 0.2838326394557953\n",
      "[step: 2681] loss: 0.2879461646080017\n",
      "[step: 2682] loss: 0.2876185178756714\n",
      "[step: 2683] loss: 0.2835365831851959\n",
      "[step: 2684] loss: 0.2841624617576599\n",
      "[step: 2685] loss: 0.2866935133934021\n",
      "[step: 2686] loss: 0.28495216369628906\n",
      "[step: 2687] loss: 0.2827163636684418\n",
      "[step: 2688] loss: 0.2840346693992615\n",
      "[step: 2689] loss: 0.28502845764160156\n",
      "[step: 2690] loss: 0.28330323100090027\n",
      "[step: 2691] loss: 0.2824516296386719\n",
      "[step: 2692] loss: 0.2836952805519104\n",
      "[step: 2693] loss: 0.2838466167449951\n",
      "[step: 2694] loss: 0.28252750635147095\n",
      "[step: 2695] loss: 0.2824062705039978\n",
      "[step: 2696] loss: 0.28324639797210693\n",
      "[step: 2697] loss: 0.28294771909713745\n",
      "[step: 2698] loss: 0.28208059072494507\n",
      "[step: 2699] loss: 0.2822272777557373\n",
      "[step: 2700] loss: 0.28271162509918213\n",
      "[step: 2701] loss: 0.2823575735092163\n",
      "[step: 2702] loss: 0.2818329930305481\n",
      "[step: 2703] loss: 0.2820173501968384\n",
      "[step: 2704] loss: 0.28228291869163513\n",
      "[step: 2705] loss: 0.2819698750972748\n",
      "[step: 2706] loss: 0.28163090348243713\n",
      "[step: 2707] loss: 0.2817605137825012\n",
      "[step: 2708] loss: 0.2818981111049652\n",
      "[step: 2709] loss: 0.28166598081588745\n",
      "[step: 2710] loss: 0.28143900632858276\n",
      "[step: 2711] loss: 0.28151974081993103\n",
      "[step: 2712] loss: 0.2816024720668793\n",
      "[step: 2713] loss: 0.28144019842147827\n",
      "[step: 2714] loss: 0.2812746465206146\n",
      "[step: 2715] loss: 0.28130748867988586\n",
      "[step: 2716] loss: 0.2813541293144226\n",
      "[step: 2717] loss: 0.28124433755874634\n",
      "[step: 2718] loss: 0.28111448884010315\n",
      "[step: 2719] loss: 0.28111279010772705\n",
      "[step: 2720] loss: 0.2811421751976013\n",
      "[step: 2721] loss: 0.2810736298561096\n",
      "[step: 2722] loss: 0.2809712290763855\n",
      "[step: 2723] loss: 0.2809440493583679\n",
      "[step: 2724] loss: 0.28095579147338867\n",
      "[step: 2725] loss: 0.2809135317802429\n",
      "[step: 2726] loss: 0.2808314859867096\n",
      "[step: 2727] loss: 0.28078725934028625\n",
      "[step: 2728] loss: 0.28078290820121765\n",
      "[step: 2729] loss: 0.2807566523551941\n",
      "[step: 2730] loss: 0.28069472312927246\n",
      "[step: 2731] loss: 0.28064393997192383\n",
      "[step: 2732] loss: 0.2806248068809509\n",
      "[step: 2733] loss: 0.28060442209243774\n",
      "[step: 2734] loss: 0.2805585265159607\n",
      "[step: 2735] loss: 0.28050780296325684\n",
      "[step: 2736] loss: 0.2804766297340393\n",
      "[step: 2737] loss: 0.28045523166656494\n",
      "[step: 2738] loss: 0.28042083978652954\n",
      "[step: 2739] loss: 0.280375599861145\n",
      "[step: 2740] loss: 0.2803378403186798\n",
      "[step: 2741] loss: 0.2803117334842682\n",
      "[step: 2742] loss: 0.28028351068496704\n",
      "[step: 2743] loss: 0.28024527430534363\n",
      "[step: 2744] loss: 0.28020578622817993\n",
      "[step: 2745] loss: 0.28017404675483704\n",
      "[step: 2746] loss: 0.2801463305950165\n",
      "[step: 2747] loss: 0.2801138758659363\n",
      "[step: 2748] loss: 0.28007692098617554\n",
      "[step: 2749] loss: 0.2800421714782715\n",
      "[step: 2750] loss: 0.2800121307373047\n",
      "[step: 2751] loss: 0.2799823582172394\n",
      "[step: 2752] loss: 0.27994900941848755\n",
      "[step: 2753] loss: 0.27991440892219543\n",
      "[step: 2754] loss: 0.27988165616989136\n",
      "[step: 2755] loss: 0.27985137701034546\n",
      "[step: 2756] loss: 0.27982062101364136\n",
      "[step: 2757] loss: 0.2797878384590149\n",
      "[step: 2758] loss: 0.279754638671875\n",
      "[step: 2759] loss: 0.2797229588031769\n",
      "[step: 2760] loss: 0.27969253063201904\n",
      "[step: 2761] loss: 0.27966147661209106\n",
      "[step: 2762] loss: 0.27962931990623474\n",
      "[step: 2763] loss: 0.2795971632003784\n",
      "[step: 2764] loss: 0.27956587076187134\n",
      "[step: 2765] loss: 0.2795354127883911\n",
      "[step: 2766] loss: 0.279504656791687\n",
      "[step: 2767] loss: 0.2794729471206665\n",
      "[step: 2768] loss: 0.2794416844844818\n",
      "[step: 2769] loss: 0.27941063046455383\n",
      "[step: 2770] loss: 0.27938026189804077\n",
      "[step: 2771] loss: 0.2793497145175934\n",
      "[step: 2772] loss: 0.2793188691139221\n",
      "[step: 2773] loss: 0.2792878746986389\n",
      "[step: 2774] loss: 0.2792571187019348\n",
      "[step: 2775] loss: 0.27922695875167847\n",
      "[step: 2776] loss: 0.27919676899909973\n",
      "[step: 2777] loss: 0.2791662812232971\n",
      "[step: 2778] loss: 0.2791356146335602\n",
      "[step: 2779] loss: 0.2791054844856262\n",
      "[step: 2780] loss: 0.2790752649307251\n",
      "[step: 2781] loss: 0.27904531359672546\n",
      "[step: 2782] loss: 0.27901533246040344\n",
      "[step: 2783] loss: 0.2789852023124695\n",
      "[step: 2784] loss: 0.2789549231529236\n",
      "[step: 2785] loss: 0.2789252996444702\n",
      "[step: 2786] loss: 0.27889567613601685\n",
      "[step: 2787] loss: 0.2788660526275635\n",
      "[step: 2788] loss: 0.27883628010749817\n",
      "[step: 2789] loss: 0.2788063883781433\n",
      "[step: 2790] loss: 0.27877676486968994\n",
      "[step: 2791] loss: 0.27874743938446045\n",
      "[step: 2792] loss: 0.27871808409690857\n",
      "[step: 2793] loss: 0.2786887288093567\n",
      "[step: 2794] loss: 0.2786593437194824\n",
      "[step: 2795] loss: 0.2786300480365753\n",
      "[step: 2796] loss: 0.2786007225513458\n",
      "[step: 2797] loss: 0.27857163548469543\n",
      "[step: 2798] loss: 0.27854254841804504\n",
      "[step: 2799] loss: 0.27851346135139465\n",
      "[step: 2800] loss: 0.2784845232963562\n",
      "[step: 2801] loss: 0.2784556746482849\n",
      "[step: 2802] loss: 0.27842673659324646\n",
      "[step: 2803] loss: 0.27839797735214233\n",
      "[step: 2804] loss: 0.27836909890174866\n",
      "[step: 2805] loss: 0.2783404588699341\n",
      "[step: 2806] loss: 0.27831190824508667\n",
      "[step: 2807] loss: 0.2782832384109497\n",
      "[step: 2808] loss: 0.27825450897216797\n",
      "[step: 2809] loss: 0.2782261371612549\n",
      "[step: 2810] loss: 0.27819761633872986\n",
      "[step: 2811] loss: 0.2781693637371063\n",
      "[step: 2812] loss: 0.27814096212387085\n",
      "[step: 2813] loss: 0.2781127095222473\n",
      "[step: 2814] loss: 0.2780846357345581\n",
      "[step: 2815] loss: 0.27805638313293457\n",
      "[step: 2816] loss: 0.27802810072898865\n",
      "[step: 2817] loss: 0.27800023555755615\n",
      "[step: 2818] loss: 0.2779723107814789\n",
      "[step: 2819] loss: 0.27794426679611206\n",
      "[step: 2820] loss: 0.27791646122932434\n",
      "[step: 2821] loss: 0.27788853645324707\n",
      "[step: 2822] loss: 0.27786070108413696\n",
      "[step: 2823] loss: 0.2778330445289612\n",
      "[step: 2824] loss: 0.2778051495552063\n",
      "[step: 2825] loss: 0.27777761220932007\n",
      "[step: 2826] loss: 0.2777499258518219\n",
      "[step: 2827] loss: 0.2777225375175476\n",
      "[step: 2828] loss: 0.2776948809623718\n",
      "[step: 2829] loss: 0.2776675820350647\n",
      "[step: 2830] loss: 0.2776399850845337\n",
      "[step: 2831] loss: 0.2776127755641937\n",
      "[step: 2832] loss: 0.27758535742759705\n",
      "[step: 2833] loss: 0.2775580585002899\n",
      "[step: 2834] loss: 0.2775307893753052\n",
      "[step: 2835] loss: 0.27750372886657715\n",
      "[step: 2836] loss: 0.2774765193462372\n",
      "[step: 2837] loss: 0.2774496078491211\n",
      "[step: 2838] loss: 0.2774224877357483\n",
      "[step: 2839] loss: 0.2773956060409546\n",
      "[step: 2840] loss: 0.2773686349391937\n",
      "[step: 2841] loss: 0.2773416340351105\n",
      "[step: 2842] loss: 0.2773148715496063\n",
      "[step: 2843] loss: 0.27728816866874695\n",
      "[step: 2844] loss: 0.27726131677627563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2845] loss: 0.2772345542907715\n",
      "[step: 2846] loss: 0.27720800042152405\n",
      "[step: 2847] loss: 0.2771814465522766\n",
      "[step: 2848] loss: 0.27715468406677246\n",
      "[step: 2849] loss: 0.27712833881378174\n",
      "[step: 2850] loss: 0.2771018147468567\n",
      "[step: 2851] loss: 0.2770754396915436\n",
      "[step: 2852] loss: 0.27704888582229614\n",
      "[step: 2853] loss: 0.27702271938323975\n",
      "[step: 2854] loss: 0.27699631452560425\n",
      "[step: 2855] loss: 0.2769702672958374\n",
      "[step: 2856] loss: 0.2769439220428467\n",
      "[step: 2857] loss: 0.2769179344177246\n",
      "[step: 2858] loss: 0.2768917679786682\n",
      "[step: 2859] loss: 0.2768656015396118\n",
      "[step: 2860] loss: 0.2768396735191345\n",
      "[step: 2861] loss: 0.2768135666847229\n",
      "[step: 2862] loss: 0.2767876386642456\n",
      "[step: 2863] loss: 0.27676188945770264\n",
      "[step: 2864] loss: 0.27673596143722534\n",
      "[step: 2865] loss: 0.27671003341674805\n",
      "[step: 2866] loss: 0.27668437361717224\n",
      "[step: 2867] loss: 0.27665847539901733\n",
      "[step: 2868] loss: 0.2766329050064087\n",
      "[step: 2869] loss: 0.2766072154045105\n",
      "[step: 2870] loss: 0.27658161520957947\n",
      "[step: 2871] loss: 0.2765560746192932\n",
      "[step: 2872] loss: 0.2765304744243622\n",
      "[step: 2873] loss: 0.2765049934387207\n",
      "[step: 2874] loss: 0.27647969126701355\n",
      "[step: 2875] loss: 0.2764543294906616\n",
      "[step: 2876] loss: 0.27642861008644104\n",
      "[step: 2877] loss: 0.2764035761356354\n",
      "[step: 2878] loss: 0.2763783633708954\n",
      "[step: 2879] loss: 0.27635306119918823\n",
      "[step: 2880] loss: 0.2763277292251587\n",
      "[step: 2881] loss: 0.27630263566970825\n",
      "[step: 2882] loss: 0.2762775719165802\n",
      "[step: 2883] loss: 0.27625250816345215\n",
      "[step: 2884] loss: 0.2762274742126465\n",
      "[step: 2885] loss: 0.27620238065719604\n",
      "[step: 2886] loss: 0.27617746591567993\n",
      "[step: 2887] loss: 0.27615249156951904\n",
      "[step: 2888] loss: 0.2761276364326477\n",
      "[step: 2889] loss: 0.2761027216911316\n",
      "[step: 2890] loss: 0.2760779857635498\n",
      "[step: 2891] loss: 0.27605319023132324\n",
      "[step: 2892] loss: 0.2760288119316101\n",
      "[step: 2893] loss: 0.27600404620170593\n",
      "[step: 2894] loss: 0.27597957849502563\n",
      "[step: 2895] loss: 0.2759552001953125\n",
      "[step: 2896] loss: 0.2759310007095337\n",
      "[step: 2897] loss: 0.2759070098400116\n",
      "[step: 2898] loss: 0.27588361501693726\n",
      "[step: 2899] loss: 0.27586066722869873\n",
      "[step: 2900] loss: 0.27583903074264526\n",
      "[step: 2901] loss: 0.27581921219825745\n",
      "[step: 2902] loss: 0.27580299973487854\n",
      "[step: 2903] loss: 0.27579280734062195\n",
      "[step: 2904] loss: 0.2757927179336548\n",
      "[step: 2905] loss: 0.27581167221069336\n",
      "[step: 2906] loss: 0.2758641242980957\n",
      "[step: 2907] loss: 0.2759787440299988\n",
      "[step: 2908] loss: 0.2762061357498169\n",
      "[step: 2909] loss: 0.27664828300476074\n",
      "[step: 2910] loss: 0.2774817943572998\n",
      "[step: 2911] loss: 0.2790847420692444\n",
      "[step: 2912] loss: 0.2820851802825928\n",
      "[step: 2913] loss: 0.2879524230957031\n",
      "[step: 2914] loss: 0.298885315656662\n",
      "[step: 2915] loss: 0.3208206295967102\n",
      "[step: 2916] loss: 0.3606467843055725\n",
      "[step: 2917] loss: 0.44239675998687744\n",
      "[step: 2918] loss: 0.5769635438919067\n",
      "[step: 2919] loss: 0.8474612236022949\n",
      "[step: 2920] loss: 1.1559512615203857\n",
      "[step: 2921] loss: 1.637310266494751\n",
      "[step: 2922] loss: 1.5313256978988647\n",
      "[step: 2923] loss: 1.1222386360168457\n",
      "[step: 2924] loss: 0.4082767069339752\n",
      "[step: 2925] loss: 0.4170101284980774\n",
      "[step: 2926] loss: 0.8691084384918213\n",
      "[step: 2927] loss: 0.7012753486633301\n",
      "[step: 2928] loss: 0.3288664221763611\n",
      "[step: 2929] loss: 0.43975594639778137\n",
      "[step: 2930] loss: 0.6181619167327881\n",
      "[step: 2931] loss: 0.4225277900695801\n",
      "[step: 2932] loss: 0.31084248423576355\n",
      "[step: 2933] loss: 0.4999105632305145\n",
      "[step: 2934] loss: 0.4889357388019562\n",
      "[step: 2935] loss: 0.3070090413093567\n",
      "[step: 2936] loss: 0.3926410675048828\n",
      "[step: 2937] loss: 0.47762835025787354\n",
      "[step: 2938] loss: 0.33147603273391724\n",
      "[step: 2939] loss: 0.325337678194046\n",
      "[step: 2940] loss: 0.4202776849269867\n",
      "[step: 2941] loss: 0.34032657742500305\n",
      "[step: 2942] loss: 0.2939351201057434\n",
      "[step: 2943] loss: 0.36587321758270264\n",
      "[step: 2944] loss: 0.34043243527412415\n",
      "[step: 2945] loss: 0.2863389849662781\n",
      "[step: 2946] loss: 0.326779305934906\n",
      "[step: 2947] loss: 0.3343106210231781\n",
      "[step: 2948] loss: 0.2880491614341736\n",
      "[step: 2949] loss: 0.29743295907974243\n",
      "[step: 2950] loss: 0.3230443298816681\n",
      "[step: 2951] loss: 0.2979520261287689\n",
      "[step: 2952] loss: 0.2815335690975189\n",
      "[step: 2953] loss: 0.30496442317962646\n",
      "[step: 2954] loss: 0.3076320290565491\n",
      "[step: 2955] loss: 0.2841399908065796\n",
      "[step: 2956] loss: 0.2865275740623474\n",
      "[step: 2957] loss: 0.3023013770580292\n",
      "[step: 2958] loss: 0.2931312918663025\n",
      "[step: 2959] loss: 0.27992233633995056\n",
      "[step: 2960] loss: 0.28742027282714844\n",
      "[step: 2961] loss: 0.2938137352466583\n",
      "[step: 2962] loss: 0.2844383716583252\n",
      "[step: 2963] loss: 0.2796480357646942\n",
      "[step: 2964] loss: 0.28699374198913574\n",
      "[step: 2965] loss: 0.2878512740135193\n",
      "[step: 2966] loss: 0.2801722586154938\n",
      "[step: 2967] loss: 0.27991294860839844\n",
      "[step: 2968] loss: 0.2848763167858124\n",
      "[step: 2969] loss: 0.2827858030796051\n",
      "[step: 2970] loss: 0.27816298604011536\n",
      "[step: 2971] loss: 0.2797468900680542\n",
      "[step: 2972] loss: 0.2824931740760803\n",
      "[step: 2973] loss: 0.28019922971725464\n",
      "[step: 2974] loss: 0.2776576578617096\n",
      "[step: 2975] loss: 0.27931445837020874\n",
      "[step: 2976] loss: 0.28064054250717163\n",
      "[step: 2977] loss: 0.27859750390052795\n",
      "[step: 2978] loss: 0.2773219347000122\n",
      "[step: 2979] loss: 0.278705358505249\n",
      "[step: 2980] loss: 0.279217928647995\n",
      "[step: 2981] loss: 0.2777472138404846\n",
      "[step: 2982] loss: 0.2771990895271301\n",
      "[step: 2983] loss: 0.27819526195526123\n",
      "[step: 2984] loss: 0.27831149101257324\n",
      "[step: 2985] loss: 0.27724459767341614\n",
      "[step: 2986] loss: 0.2769932746887207\n",
      "[step: 2987] loss: 0.2776508629322052\n",
      "[step: 2988] loss: 0.2776224613189697\n",
      "[step: 2989] loss: 0.27690836787223816\n",
      "[step: 2990] loss: 0.27676525712013245\n",
      "[step: 2991] loss: 0.2771907448768616\n",
      "[step: 2992] loss: 0.2771722078323364\n",
      "[step: 2993] loss: 0.27668240666389465\n",
      "[step: 2994] loss: 0.27653729915618896\n",
      "[step: 2995] loss: 0.2767910361289978\n",
      "[step: 2996] loss: 0.27679771184921265\n",
      "[step: 2997] loss: 0.2764739692211151\n",
      "[step: 2998] loss: 0.27632713317871094\n",
      "[step: 2999] loss: 0.2764725983142853\n",
      "RMSE: 0.020987367257475853\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: X_train, Y: y_train})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    predicted_BTC_price = sess.run(Y_pred, feed_dict={X: X_test})\n",
    "    rmse_val = sess.run(rmse, feed_dict={\n",
    "                    targets: y_test, predictions: predicted_BTC_price})\n",
    "    print(\"RMSE: {}\".format(rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "chubby-witch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test, color = 'red', label = 'Real BTC Value', alpha = 0.3)\n",
    "plt.plot(predicted_BTC_price, color = 'blue', label = 'Predicted BTC Value', alpha = 0.3)\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"BTC Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-banner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
